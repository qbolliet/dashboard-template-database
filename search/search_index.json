{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Database schema builder for dashboard template","text":"<p>This package contains utilities to build a specific generic database scheme from a table.</p>"},{"location":"#objectives","title":"Objectives","text":"<p>This package is built to create :</p> <ul> <li> <p>a generic schema to store a table as a database :</p> <ul> <li>the <code>metadata</code> table references general information (label, type etc ...) about each dataframe column ;</li> <li>the <code>dimension</code> tables associate each modality of a categorical variable to an <code>id</code> used in the <code>fact</code> table ;</li> <li>the <code>fact</code> table reflects the information in the original table ;</li> </ul> </li> <li> <p>export this schema a relational DuckDB database.</p> </li> </ul> <p></p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#package-and-dependencies","title":"Package and dependencies","text":"<pre><code>git clone https://github.com/qbolliet/dashboard-template-database.git\npoetry install\n</code></pre> <p>The package is then usable as any other python package.</p>"},{"location":"#parametrisation","title":"Parametrisation","text":"<p>File in the <code>config.yaml</code> file : <pre><code>INPUT_DATA : '../data/df_origin.csv'\nOUTPUT_DATA : '../outputs/database.db'\nTHRESHOLD : 200\n</code></pre></p>"},{"location":"#documentation","title":"Documentation","text":"<p>To visualize the documentation : <pre><code>poetry install --with docs\n</code></pre></p> <pre><code>mkdocs build --port 5000\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Here's an example of how to use the functions in the package:</p> <pre><code>import pandas as pd\nfrom dashboard_template_database.builders.schema import SchemaBuilder\nfrom dashboard_template_database.builders.tables import DuckdbTablesBuilder\nfrom dashboard_template_database.loaders.local.loader import Loader\n\n# Load a sample DataFrame\nloader = Loader()\nsample_data = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['Paris', 'Berlin', 'Madrid']\n})\n\n# Initialize the SchemaBuilder\nschema_builder = SchemaBuilder(df=sample_data, categorical_threshold=3)\n\n# Build the schema\nmetadata, dimension_tables, fact_table = schema_builder.build()\n\n# Initialize the DuckDB tables builder\nduckdb_builder = DuckdbTablesBuilder(df=sample_data)\n\n# Create the schema in DuckDB\nduckdb_builder.build_duckdb_schema()\n\n# Display the schema in DuckDB\nduckdb_builder.display_schema()\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The package is licensed under the MIT License.</p>"},{"location":"api/builders/schema/SchemaBuilder/","title":"SchemaBuilder","text":""},{"location":"api/builders/schema/SchemaBuilder/#dashboard_template_database.builders.schema.SchemaBuilder","title":"SchemaBuilder","text":"<p>A class to automate the creation of metadata, dimension tables, and fact tables  from a given pandas DataFrame, primarily for use in data warehousing.</p> <p>Attributes:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The input dataset.</p> <code>categorical_threshold</code> <code>int</code> <p>Threshold for determining if a column is categorical                           based on the number of unique modalities.</p> <code>logger</code> <code>Logger</code> <p>Logger instance for tracking processing steps.</p> <p>Methods:</p> Name Description <code>build</code> <p>Execute the full pipeline to create metadata, dimension tables, and a fact table.</p> <code>create_dimension_tables</code> <p>Generate dimension tables for categorical columns in the dataset.</p> <code>create_fact_table</code> <p>Generate a fact table by replacing categorical values with corresponding IDs.</p> <code>create_metadata_table</code> <p>Automatically infer metadata for the DataFrame's columns, including types, labels, </p> Source code in <code>dashboard_template_database/builders/schema.py</code> <pre><code>class SchemaBuilder:\n    \"\"\"\n    A class to automate the creation of metadata, dimension tables, and fact tables \n    from a given pandas DataFrame, primarily for use in data warehousing.\n\n    Attributes:\n        df (pd.DataFrame): The input dataset.\n        categorical_threshold (int): Threshold for determining if a column is categorical \n                                     based on the number of unique modalities.\n        logger (logging.Logger): Logger instance for tracking processing steps.\n    \"\"\"\n\n    # Initialisation\n    def __init__(self, df: pd.DataFrame, categorical_threshold: Optional[int] = 50, log_filename: Optional[os.PathLike] = os.path.join(FILE_PATH.parents[2], \"logs/schema_builder.log\")) -&gt; None:\n        \"\"\"\n        Initialize the SchemaBuilder with a DataFrame and optional parameters.\n\n        Args:\n            df (pd.DataFrame): The input dataset.\n            categorical_threshold (int, optional): Maximum number of unique values \n                                                   for a column to be considered categorical. Defaults to 50.\n            log_filename (os.PathLike, optional): Path to the log file. Defaults to \n                                                  a file named `schema_builder.log` in a logs directory.\n        \"\"\"\n        # Initialisation des arguments\n        # Jeu de donn\u00e9es\n        self.df = df\n        # Initialisation du seuil au de\u00e7\u00e0 duquel les modalit\u00e9s d'une variable cat\u00e9gorielle ne sont plus export\u00e9es dans \n        self.categorical_threshold = categorical_threshold\n        # Initialisation du logger\n        self.logger = _init_logger(filename=log_filename)\n\n    # M\u00e9thode inf\u00e9rant le type des colonnes du jeu de donn\u00e9es \n    def create_metadata_table(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; Dict:\n        \"\"\"\n        Automatically infer metadata for the DataFrame's columns, including types, labels, \n        and categorical attributes.\n\n        Args:\n            column_labels (dict, optional): A dictionary mapping column names to labels.\n                                            Defaults to None.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing metadata for each column in the input dataset.\n        \"\"\"\n        # Initialisation de la liste des m\u00e9ta-donn\u00e9es\n        list_metadata = []\n        # Parcours des colonnes du jeu de donn\u00e9es\n        for col in self.df.columns:\n            # Extraction du type de la colonne\n            dtype = str(self.df[col].dtype)\n            # Initialisation des m\u00e9ta-donn\u00e9es associ\u00e9es \u00e0 la colonne\n            if column_labels is not None :\n                metadata = {\n                    'name': col,\n                    'label': column_labels[col] if col in column_labels.keys() else col.replace('_', ' ').title(),\n                    'python_type': dtype,\n                    'sql_type': self._map_python_to_sql_type(dtype),\n                    'is_categorical': False,\n                    # 'modalities': None\n                }\n            else :\n                metadata = {\n                    'name': col,\n                    'label': col.replace('_', ' ').title(),\n                    'python_type': dtype,\n                    'sql_type': self._map_python_to_sql_type(dtype),\n                    'is_categorical': False,\n                    # 'modalities': None\n                }\n\n            # Logging\n            self.logger.info(f\"Successfully extracted meta-data from column '{col}'\")\n\n            # Si la colonne est object\n            if dtype == 'object' :\n                # Calcul du nombre de modalit\u00e9s\n                n_modalities = self.df[col].nunique()\n                # Si le nombre de modalit\u00e9s dans la colonne est inf\u00e9rieur au seuil, la variable est cat\u00e9gorielle\n                if  n_modalities &lt;= self.categorical_threshold:\n                    # Mise \u00e0 jour du type de la variable\n                    metadata['is_categorical'] = True\n                    # Logging\n                    self.logger.info(f\"The column '{col}' is of type 'object' and the number of modalities {n_modalities} satisfies the categorical threshold criteria {self.categorical_threshold}\")\n                    # Mise \u00e0 jour des modalit\u00e9s\n                    # metadata['modalities'] = str(self.df[col].dropna().unique().tolist())\n                else :\n                    # Logging\n                    self.logger.warning(f\"The column '{col}' is  of type 'object' but the number of modalities {n_modalities} exceeds the categorical threshold criteria {self.categorical_threshold}\")\n\n            # Ajout au dictionnaire\n            list_metadata.append(metadata)            \n\n        # Cr\u00e9ation d'un DataFrame\n        self.df_metadata = pd.DataFrame.from_dict(list_metadata).sort_values(by='label', ascending=True, ignore_index=True)\n\n        # Logging\n        self.logger.info(\"Successfully built the meta-data DataFrame\")\n\n        return self.df_metadata\n\n    # Correspondance entre les types \"python\" et \"SQL\"\n    @staticmethod\n    def _map_python_to_sql_type(dtype: str) -&gt; str:\n        \"\"\"\n        Map Python data types to SQL-compatible data types.\n\n        Args:\n            dtype (str): The Python data type as a string.\n\n        Returns:\n            str: The corresponding SQL data type.\n        \"\"\"\n        # /!\\ Peut \u00eatre mis dans un json de param\u00e8tres\n        # Dictionnaire des correspondance entre les types Python et SQL\n        type_mapping = {\n            'object': 'VARCHAR',\n            'int64': 'INTEGER',\n            'float64': 'DOUBLE',\n            'datetime64[ns]': 'TIMESTAMP',\n            'bool': 'BOOLEAN'\n        }\n        return type_mapping.get(dtype, 'VARCHAR')\n\n    # M\u00e9thode cr\u00e9ant la dimension table\n    def create_dimension_tables(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; Dict[str, pd.DataFrame] :\n        \"\"\"\n        Generate dimension tables for categorical columns in the dataset.\n\n        Args:\n            column_labels (dict, optional): A dictionary mapping column names to labels.\n                                            Defaults to None.\n\n        Returns:\n            dict: A dictionary of DataFrames, where keys are column names and values are \n                  the corresponding dimension tables.\n        \"\"\"\n        # Cr\u00e9ation d'une table de m\u00e9ta-donn\u00e9es si cette-derni\u00e8re n'existe pas d\u00e9j\u00e0\n        if not hasattr(self, 'metadata') :\n            _ = self.create_metadata_table(column_labels=column_labels)\n\n        # Initialisation du dictionnaire des tables de dimension\n        self.dimension_tables = {}\n\n        # Parcours des tables de dimensions\n        for categorical_dimension in self.df_metadata.loc[self.df_metadata['is_categorical'], 'name'] :\n            # Extraction des modalit\u00e9s\n            self.dimension_tables[categorical_dimension] = pd.Series(self.df[categorical_dimension].unique(), name='label').to_frame().reset_index(names='value').sort_values(by='label', ascending=True, ignore_index=True)\n            # Logging\n            self.logger.info(f\"Successfully built dimension table for '{categorical_dimension}'\")\n\n        # Logging\n        self.logger.info(\"Successfully built dimension tables\")\n\n        return self.dimension_tables\n\n    # M\u00e9thode cr\u00e9ant la table des informations\n    def create_fact_table(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate a fact table by replacing categorical values with corresponding IDs.\n\n        Args:\n            column_labels (dict, optional): A dictionary mapping column names to labels.\n                                            Defaults to None.\n\n        Returns:\n            pd.DataFrame: The fact table with categorical values replaced by IDs.\n        \"\"\"\n        # Cr\u00e9ation des tables de dimensions si ces-derni\u00e8res n'existent pas\n        if not hasattr(self, 'dimension_tables') :\n            _ = self.create_dimension_tables(column_labels=column_labels)\n\n        # Initialisation de la table des informations\n        self.df_fact = self.df.copy()\n\n        # Remplacement des labels par leur valeur\n        for column in self.dimension_tables.keys() :\n            # Construction du dictionnaire de passage\n            dict_label_value = {d['label'] : d['value'] for d in self.dimension_tables[column].to_dict(orient='records')}\n            # Remplacement des valeurs\n            self.df_fact[column] = self.df_fact[column].replace(dict_label_value)\n            # Logging\n            self.logger.info(f\"Successfully replace modalities by ids in column '{column}'\")\n\n        # Logging\n        self.logger.info(\"Successfully built fact table\")\n\n        return self.df_fact\n\n    # M\u00e9thode cr\u00e9ant les diff\u00e9rentes tables\n    def build(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; Tuple[pd.DataFrame, Dict[str, pd.DataFrame], pd.DataFrame] :\n        \"\"\"\n        Execute the full pipeline to create metadata, dimension tables, and a fact table.\n\n        Args:\n            column_labels (dict, optional): A dictionary mapping column names to labels.\n                                            Defaults to None.\n\n        Returns:\n            tuple: A tuple containing:\n                   - Metadata DataFrame.\n                   - Dictionary of dimension tables.\n                   - Fact table DataFrame.\n        \"\"\"\n        # Cr\u00e9ation de la table des m\u00e9ta-donn\u00e9es\n        _ = self.create_metadata_table(column_labels=column_labels)\n        # Cr\u00e9ation des tables de dimension\n        _ = self.create_dimension_tables(column_labels=column_labels)\n        # Cr\u00e9ation des tables d'informations\n        _ = self.create_fact_table(column_labels=column_labels)\n\n        return self.df_metadata, self.dimension_tables, self.df_fact\n</code></pre>"},{"location":"api/builders/schema/SchemaBuilder/#dashboard_template_database.builders.schema.SchemaBuilder.build","title":"build","text":"<pre><code>build(column_labels: Optional[Union[Dict[str, str], None]] = None) -&gt; Tuple[DataFrame, Dict[str, DataFrame], DataFrame]\n</code></pre> <p>Execute the full pipeline to create metadata, dimension tables, and a fact table.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary mapping column names to labels.                             Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, Dict[str, DataFrame], DataFrame]</code> <p>A tuple containing:    - Metadata DataFrame.    - Dictionary of dimension tables.    - Fact table DataFrame.</p> Source code in <code>dashboard_template_database/builders/schema.py</code> <pre><code>def build(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; Tuple[pd.DataFrame, Dict[str, pd.DataFrame], pd.DataFrame] :\n    \"\"\"\n    Execute the full pipeline to create metadata, dimension tables, and a fact table.\n\n    Args:\n        column_labels (dict, optional): A dictionary mapping column names to labels.\n                                        Defaults to None.\n\n    Returns:\n        tuple: A tuple containing:\n               - Metadata DataFrame.\n               - Dictionary of dimension tables.\n               - Fact table DataFrame.\n    \"\"\"\n    # Cr\u00e9ation de la table des m\u00e9ta-donn\u00e9es\n    _ = self.create_metadata_table(column_labels=column_labels)\n    # Cr\u00e9ation des tables de dimension\n    _ = self.create_dimension_tables(column_labels=column_labels)\n    # Cr\u00e9ation des tables d'informations\n    _ = self.create_fact_table(column_labels=column_labels)\n\n    return self.df_metadata, self.dimension_tables, self.df_fact\n</code></pre>"},{"location":"api/builders/schema/SchemaBuilder/#dashboard_template_database.builders.schema.SchemaBuilder.build(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/schema/SchemaBuilder/#dashboard_template_database.builders.schema.SchemaBuilder.create_dimension_tables","title":"create_dimension_tables","text":"<pre><code>create_dimension_tables(column_labels: Optional[Union[Dict[str, str], None]] = None) -&gt; Dict[str, DataFrame]\n</code></pre> <p>Generate dimension tables for categorical columns in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary mapping column names to labels.                             Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, DataFrame]</code> <p>A dictionary of DataFrames, where keys are column names and values are    the corresponding dimension tables.</p> Source code in <code>dashboard_template_database/builders/schema.py</code> <pre><code>def create_dimension_tables(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; Dict[str, pd.DataFrame] :\n    \"\"\"\n    Generate dimension tables for categorical columns in the dataset.\n\n    Args:\n        column_labels (dict, optional): A dictionary mapping column names to labels.\n                                        Defaults to None.\n\n    Returns:\n        dict: A dictionary of DataFrames, where keys are column names and values are \n              the corresponding dimension tables.\n    \"\"\"\n    # Cr\u00e9ation d'une table de m\u00e9ta-donn\u00e9es si cette-derni\u00e8re n'existe pas d\u00e9j\u00e0\n    if not hasattr(self, 'metadata') :\n        _ = self.create_metadata_table(column_labels=column_labels)\n\n    # Initialisation du dictionnaire des tables de dimension\n    self.dimension_tables = {}\n\n    # Parcours des tables de dimensions\n    for categorical_dimension in self.df_metadata.loc[self.df_metadata['is_categorical'], 'name'] :\n        # Extraction des modalit\u00e9s\n        self.dimension_tables[categorical_dimension] = pd.Series(self.df[categorical_dimension].unique(), name='label').to_frame().reset_index(names='value').sort_values(by='label', ascending=True, ignore_index=True)\n        # Logging\n        self.logger.info(f\"Successfully built dimension table for '{categorical_dimension}'\")\n\n    # Logging\n    self.logger.info(\"Successfully built dimension tables\")\n\n    return self.dimension_tables\n</code></pre>"},{"location":"api/builders/schema/SchemaBuilder/#dashboard_template_database.builders.schema.SchemaBuilder.create_dimension_tables(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/schema/SchemaBuilder/#dashboard_template_database.builders.schema.SchemaBuilder.create_fact_table","title":"create_fact_table","text":"<pre><code>create_fact_table(column_labels: Optional[Union[Dict[str, str], None]] = None) -&gt; DataFrame\n</code></pre> <p>Generate a fact table by replacing categorical values with corresponding IDs.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary mapping column names to labels.                             Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The fact table with categorical values replaced by IDs.</p> Source code in <code>dashboard_template_database/builders/schema.py</code> <pre><code>def create_fact_table(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a fact table by replacing categorical values with corresponding IDs.\n\n    Args:\n        column_labels (dict, optional): A dictionary mapping column names to labels.\n                                        Defaults to None.\n\n    Returns:\n        pd.DataFrame: The fact table with categorical values replaced by IDs.\n    \"\"\"\n    # Cr\u00e9ation des tables de dimensions si ces-derni\u00e8res n'existent pas\n    if not hasattr(self, 'dimension_tables') :\n        _ = self.create_dimension_tables(column_labels=column_labels)\n\n    # Initialisation de la table des informations\n    self.df_fact = self.df.copy()\n\n    # Remplacement des labels par leur valeur\n    for column in self.dimension_tables.keys() :\n        # Construction du dictionnaire de passage\n        dict_label_value = {d['label'] : d['value'] for d in self.dimension_tables[column].to_dict(orient='records')}\n        # Remplacement des valeurs\n        self.df_fact[column] = self.df_fact[column].replace(dict_label_value)\n        # Logging\n        self.logger.info(f\"Successfully replace modalities by ids in column '{column}'\")\n\n    # Logging\n    self.logger.info(\"Successfully built fact table\")\n\n    return self.df_fact\n</code></pre>"},{"location":"api/builders/schema/SchemaBuilder/#dashboard_template_database.builders.schema.SchemaBuilder.create_fact_table(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/schema/SchemaBuilder/#dashboard_template_database.builders.schema.SchemaBuilder.create_metadata_table","title":"create_metadata_table","text":"<pre><code>create_metadata_table(column_labels: Optional[Union[Dict[str, str], None]] = None) -&gt; Dict\n</code></pre> <p>Automatically infer metadata for the DataFrame's columns, including types, labels,  and categorical attributes.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary mapping column names to labels.                             Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>pd.DataFrame: A DataFrame containing metadata for each column in the input dataset.</p> Source code in <code>dashboard_template_database/builders/schema.py</code> <pre><code>def create_metadata_table(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; Dict:\n    \"\"\"\n    Automatically infer metadata for the DataFrame's columns, including types, labels, \n    and categorical attributes.\n\n    Args:\n        column_labels (dict, optional): A dictionary mapping column names to labels.\n                                        Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing metadata for each column in the input dataset.\n    \"\"\"\n    # Initialisation de la liste des m\u00e9ta-donn\u00e9es\n    list_metadata = []\n    # Parcours des colonnes du jeu de donn\u00e9es\n    for col in self.df.columns:\n        # Extraction du type de la colonne\n        dtype = str(self.df[col].dtype)\n        # Initialisation des m\u00e9ta-donn\u00e9es associ\u00e9es \u00e0 la colonne\n        if column_labels is not None :\n            metadata = {\n                'name': col,\n                'label': column_labels[col] if col in column_labels.keys() else col.replace('_', ' ').title(),\n                'python_type': dtype,\n                'sql_type': self._map_python_to_sql_type(dtype),\n                'is_categorical': False,\n                # 'modalities': None\n            }\n        else :\n            metadata = {\n                'name': col,\n                'label': col.replace('_', ' ').title(),\n                'python_type': dtype,\n                'sql_type': self._map_python_to_sql_type(dtype),\n                'is_categorical': False,\n                # 'modalities': None\n            }\n\n        # Logging\n        self.logger.info(f\"Successfully extracted meta-data from column '{col}'\")\n\n        # Si la colonne est object\n        if dtype == 'object' :\n            # Calcul du nombre de modalit\u00e9s\n            n_modalities = self.df[col].nunique()\n            # Si le nombre de modalit\u00e9s dans la colonne est inf\u00e9rieur au seuil, la variable est cat\u00e9gorielle\n            if  n_modalities &lt;= self.categorical_threshold:\n                # Mise \u00e0 jour du type de la variable\n                metadata['is_categorical'] = True\n                # Logging\n                self.logger.info(f\"The column '{col}' is of type 'object' and the number of modalities {n_modalities} satisfies the categorical threshold criteria {self.categorical_threshold}\")\n                # Mise \u00e0 jour des modalit\u00e9s\n                # metadata['modalities'] = str(self.df[col].dropna().unique().tolist())\n            else :\n                # Logging\n                self.logger.warning(f\"The column '{col}' is  of type 'object' but the number of modalities {n_modalities} exceeds the categorical threshold criteria {self.categorical_threshold}\")\n\n        # Ajout au dictionnaire\n        list_metadata.append(metadata)            \n\n    # Cr\u00e9ation d'un DataFrame\n    self.df_metadata = pd.DataFrame.from_dict(list_metadata).sort_values(by='label', ascending=True, ignore_index=True)\n\n    # Logging\n    self.logger.info(\"Successfully built the meta-data DataFrame\")\n\n    return self.df_metadata\n</code></pre>"},{"location":"api/builders/schema/SchemaBuilder/#dashboard_template_database.builders.schema.SchemaBuilder.create_metadata_table(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/","title":"DuckdbTablesBuilder","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder","title":"DuckdbTablesBuilder","text":"<p>A class to build and manage schema tables in DuckDB.</p> <p>This class extends <code>SchemaBuilder</code> and provides functionality to create  metadata, dimension, and fact tables in DuckDB, while logging all operations.</p> <p>Attributes:</p> Name Type Description <code>conn</code> <code>DuckDBPyConnection</code> <p>DuckDB connection object.</p> <p>Methods:</p> Name Description <code>build</code> <p>Execute the full pipeline to create metadata, dimension tables, and a fact table.</p> <code>build_duckdb_schema</code> <p>Build the entire schema in DuckDB, including metadata, dimension, and fact tables.</p> <code>create_dimension_tables</code> <p>Generate dimension tables for categorical columns in the dataset.</p> <code>create_duckdb_dimension_tables</code> <p>Create dimension tables in DuckDB for categorical variables.</p> <code>create_duckdb_fact_table</code> <p>Create a fact table in DuckDB with foreign key relationships to dimension tables.</p> <code>create_duckdb_metadata_table</code> <p>Create a metadata table in DuckDB.</p> <code>create_fact_table</code> <p>Generate a fact table by replacing categorical values with corresponding IDs.</p> <code>create_metadata_table</code> <p>Automatically infer metadata for the DataFrame's columns, including types, labels, </p> <code>display_schema</code> <p>Display the structure of all tables in the DuckDB schema.</p> Source code in <code>dashboard_template_database/builders/tables.py</code> <pre><code>class DuckdbTablesBuilder(SchemaBuilder):\n    \"\"\"\n    A class to build and manage schema tables in DuckDB.\n\n    This class extends `SchemaBuilder` and provides functionality to create \n    metadata, dimension, and fact tables in DuckDB, while logging all operations.\n\n    Attributes:\n        conn (duckdb.DuckDBPyConnection): DuckDB connection object.\n    \"\"\"\n\n    # Initialisation\n    def __init__(self, df: pd.DataFrame, categorical_threshold: Optional[int] = 50, connection: Optional[duckdb.DuckDBPyConnection] = None, path : Optional[Union[os.PathLike, None]]=None, log_filename: Optional[os.PathLike] = os.path.join(FILE_PATH.parents[2], \"logs/duckdb_schema_builder.log\")):\n        \"\"\"\n        Initialize the DuckdbTablesBuilder class.\n\n        Args:\n            df (pd.DataFrame): The input dataset.\n            categorical_threshold (Optional[int]): Threshold for determining categorical variables.\n            connection (Optional[duckdb.DuckDBPyConnection]): Existing DuckDB connection. Defaults to None.\n            path (Optional[Union[os.PathLike, None]]): Path to the DuckDB database file. Defaults to None.\n            log_filename (Optional[os.PathLike]): Path to the log file. Defaults to a pre-defined path.\n\n        \"\"\"\n        # Initialisation du sch\u00e9ma\n        super().__init__(df=df, categorical_threshold=categorical_threshold, log_filename=log_filename)\n\n        # Initialisation de la connection\n        if (connection is None) &amp; (path is None) :\n            self.conn = duckdb.connect(':memory:')\n        elif (connection is None) :\n            self.conn = duckdb.connect(path)\n        else :\n            self.conn = connection\n\n    # M\u00e9thode de cr\u00e9ation de la table des m\u00e9ta-donn\u00e9es\n    def create_duckdb_metadata_table(self, table_name: Optional[str] = 'metadata', column_labels: Optional[Dict[str, str]] = None) -&gt; None:\n        \"\"\"\n        Create a metadata table in DuckDB.\n\n        Args:\n            table_name (Optional[str]): Name of the metadata table in DuckDB. Defaults to 'metadata'.\n            column_labels (Optional[Dict[str, str]]): Optional mapping of column names to labels.\n\n        \"\"\"\n        # Cr\u00e9ation de la table des m\u00e9ta-donn\u00e9es si elle n'existe pas d\u00e9j\u00e0\n        if not hasattr(self, 'df_metadata'):\n            _ = self.create_metadata_table(column_labels)\n\n        # Conversion DataFrame en table DuckDB\n        self.conn.register('temp_metadata', self.df_metadata)\n        self.conn.execute(f\"\"\"\n            CREATE TABLE {table_name} AS \n            SELECT * FROM temp_metadata\n        \"\"\")\n        self.conn.execute('DROP VIEW temp_metadata')\n\n        # Logging\n        self.logger.info(\"Successfully registered duckdb meta-data table\")\n\n    # M\u00e9thode de cr\u00e9ation des tables de dimensions\n    def create_duckdb_dimension_tables(self, table_prefix: Optional[str] = 'dim_', column_labels: Optional[Dict[str, str]] = None) -&gt; None:\n        \"\"\"\n        Create dimension tables in DuckDB for categorical variables.\n\n        Args:\n            table_prefix (Optional[str]): Prefix for dimension table names. Defaults to 'dim_'.\n            column_labels (Optional[Dict[str, str]]): Optional mapping of column names to labels.\n\n        \"\"\"\n        # Cr\u00e9ation du dictionnaire des tables de dimensions si elles n'existent pas d\u00e9j\u00e0\n        if not hasattr(self, 'dimension_tables'):\n            _ = self.create_dimension_tables(column_labels)\n\n        # Cr\u00e9ation de chaque table de dimension dans DuckDB\n        for dim_name, dim_df in self.dimension_tables.items():\n            # Initialisation du nom de la table\n            table_name = f\"{table_prefix}{dim_name}\"\n            # Enregistrement d'une vue temporaire\n            self.conn.register('temp_dim', dim_df)\n\n            # Cr\u00e9ation d'une table avec \"value\" comme cl\u00e9 primaire\n            self.conn.execute(f\"\"\"\n                CREATE TABLE {table_name} AS \n                SELECT\n                    value,\n                    label \n                FROM temp_dim\n            \"\"\")\n\n            # Ajout de la vue correspondante\n            self.conn.execute('DROP VIEW temp_dim')\n\n            # Logging\n            self.logger.info(f\"Successfully registered duckdb dimension table for {dim_name}\")\n\n    # M\u00e9thode de cr\u00e9ation de la table d'informations\n    def create_duckdb_fact_table(self, table_name: Optional[str] = 'fact_table', table_prefix: Optional[str] = 'dim_', column_labels: Optional[Dict[str, str]] = None) -&gt; None:\n        \"\"\"\n        Create a fact table in DuckDB with foreign key relationships to dimension tables.\n\n        Args:\n            table_name (Optional[str]): Name of the fact table in DuckDB. Defaults to 'fact_table'.\n            table_prefix (Optional[str]): Prefix for dimension table names. Defaults to 'dim_'.\n            column_labels (Optional[Dict[str, str]]): Optional mapping of column names to labels.\n\n        \"\"\"\n        # Cr\u00e9ation de la table d'informations si elle n'existe pas d\u00e9j\u00e0\n        if not hasattr(self, 'df_fact'):\n            _ = self.create_fact_table(column_labels)\n\n        # Enregistrement de la table comme vue temporaire\n        self.conn.register('temp_fact', self.df_fact)\n\n        # Initialisation de la liste des conditions de jointure avec les tables de dimensions\n        join_conditions = []\n        # Initialisation de la liste des colonnes \u00e0 s\u00e9lectionner\n        # select_columns = []\n\n        # Parcours des tables de dimensions correspondant \u00e0 des cl\u00e9s \u00e9trang\u00e8res dans la table d'informations\n        for dim_name in self.dimension_tables.keys():\n            # Initialisation du nom de la table\n            dim_table = f\"{table_prefix}{dim_name}\"\n\n            # Ajout des conditions de jointure sur la base de la colonne \"value\" des tables de dimensions\n            join_conditions.append(\n                f\"LEFT JOIN {dim_table} ON temp_fact.{dim_name} = {dim_table}.value\"\n            )\n\n            # Logging\n            self.logger.info(f\"Successfully created foreign key for dimension '{dim_name}'\")\n            # Ajout de la colonne qui correspond \u00e0 une cl\u00e9 \u00e9trang\u00e8re\n            # select_columns.append(f\"temp_fact.{dim_name}\")\n\n        # Ajout des colonnes des variables non cat\u00e9gorielles\n        # select_columns.extend([f\"temp_fact.{col}\" for col in self.df_fact.columns if col not in self.dimension_tables.keys()])\n\n        # Cr\u00e9ation de la table d'information\n        query = f\"\"\"\n        CREATE TABLE {table_name} AS \n        SELECT *\n        FROM temp_fact\n        {' '.join(join_conditions)}\n        \"\"\"\n\n        self.conn.execute(query)\n\n        # Ajout de la vue\n        self.conn.execute('DROP VIEW temp_fact')\n\n        # Logging\n        self.logger.info(f\"Successfully registered duckdb fact table\")\n\n    # M\u00e9thode de construction du sch\u00e9ma\n    def build_duckdb_schema(self, metadata_table: Optional[str] = 'metadata', fact_table: Optional[str] = 'fact_table', dim_table_prefix: Optional[str] = 'dim_', column_labels: Optional[Dict[str, str]] = None) -&gt; None:\n        \"\"\"\n        Build the entire schema in DuckDB, including metadata, dimension, and fact tables.\n\n        Args:\n            metadata_table (Optional[str]): Name of the metadata table. Defaults to 'metadata'.\n            fact_table (Optional[str]): Name of the fact table. Defaults to 'fact_table'.\n            dim_table_prefix (Optional[str]): Prefix for dimension tables. Defaults to 'dim_'.\n            column_labels (Optional[Dict[str, str]]): Optional mapping of column names to labels.\n\n        \"\"\"\n        # Cr\u00e9ation de la table des m\u00e9ta-donn\u00e9es\n        self.create_duckdb_metadata_table(\n            table_name=metadata_table, \n            column_labels=column_labels\n        )\n\n        # Cr\u00e9ation de la table de dimensions\n        self.create_duckdb_dimension_tables(\n            table_prefix=dim_table_prefix, \n            column_labels=column_labels\n        )\n\n        # Cr\u00e9ation de la table d'informations avec les cl\u00e9s \u00e9trang\u00e8res\n        self.create_duckdb_fact_table(\n            table_name=fact_table, \n            table_prefix=dim_table_prefix, \n            column_labels=column_labels\n        )\n\n    # M\u00e9thode d'affichage du sch\u00e9ma\n    def display_schema(self) -&gt; None:\n        \"\"\"\n        Display the structure of all tables in the DuckDB schema.\n        \"\"\"\n        # Extraction des tables\n        tables = self.conn.execute(\"SHOW TABLES\").fetchall()\n        # Logging\n        self.logger.info(\"\\n Created Tables:\")\n        # Parcours des tables\n        for table in tables:\n            # Affichage de la structure\n            self.logger.info(f\"\\n {table[0]} Structure:\")\n            # Extraction des informations relatives \u00e0 la table\n            table_info = self.conn.execute(f\"DESCRIBE {table[0]}\").fetchall()\n            # Affichage de chaque information\n            for col in table_info:\n                self.logger.info(f\"  {col[0]}: {col[1]}\")\n</code></pre>"},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.build","title":"build","text":"<pre><code>build(column_labels: Optional[Union[Dict[str, str], None]] = None) -&gt; Tuple[DataFrame, Dict[str, DataFrame], DataFrame]\n</code></pre> <p>Execute the full pipeline to create metadata, dimension tables, and a fact table.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary mapping column names to labels.                             Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, Dict[str, DataFrame], DataFrame]</code> <p>A tuple containing:    - Metadata DataFrame.    - Dictionary of dimension tables.    - Fact table DataFrame.</p> Source code in <code>dashboard_template_database/builders/schema.py</code> <pre><code>def build(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; Tuple[pd.DataFrame, Dict[str, pd.DataFrame], pd.DataFrame] :\n    \"\"\"\n    Execute the full pipeline to create metadata, dimension tables, and a fact table.\n\n    Args:\n        column_labels (dict, optional): A dictionary mapping column names to labels.\n                                        Defaults to None.\n\n    Returns:\n        tuple: A tuple containing:\n               - Metadata DataFrame.\n               - Dictionary of dimension tables.\n               - Fact table DataFrame.\n    \"\"\"\n    # Cr\u00e9ation de la table des m\u00e9ta-donn\u00e9es\n    _ = self.create_metadata_table(column_labels=column_labels)\n    # Cr\u00e9ation des tables de dimension\n    _ = self.create_dimension_tables(column_labels=column_labels)\n    # Cr\u00e9ation des tables d'informations\n    _ = self.create_fact_table(column_labels=column_labels)\n\n    return self.df_metadata, self.dimension_tables, self.df_fact\n</code></pre>"},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.build(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.build_duckdb_schema","title":"build_duckdb_schema","text":"<pre><code>build_duckdb_schema(metadata_table: Optional[str] = 'metadata', fact_table: Optional[str] = 'fact_table', dim_table_prefix: Optional[str] = 'dim_', column_labels: Optional[Dict[str, str]] = None) -&gt; None\n</code></pre> <p>Build the entire schema in DuckDB, including metadata, dimension, and fact tables.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[str]</code> <p>Name of the metadata table. Defaults to 'metadata'.</p> <code>'metadata'</code> <code>Optional[str]</code> <p>Name of the fact table. Defaults to 'fact_table'.</p> <code>'fact_table'</code> <code>Optional[str]</code> <p>Prefix for dimension tables. Defaults to 'dim_'.</p> <code>'dim_'</code> <code>Optional[Dict[str, str]]</code> <p>Optional mapping of column names to labels.</p> <code>None</code> Source code in <code>dashboard_template_database/builders/tables.py</code> <pre><code>def build_duckdb_schema(self, metadata_table: Optional[str] = 'metadata', fact_table: Optional[str] = 'fact_table', dim_table_prefix: Optional[str] = 'dim_', column_labels: Optional[Dict[str, str]] = None) -&gt; None:\n    \"\"\"\n    Build the entire schema in DuckDB, including metadata, dimension, and fact tables.\n\n    Args:\n        metadata_table (Optional[str]): Name of the metadata table. Defaults to 'metadata'.\n        fact_table (Optional[str]): Name of the fact table. Defaults to 'fact_table'.\n        dim_table_prefix (Optional[str]): Prefix for dimension tables. Defaults to 'dim_'.\n        column_labels (Optional[Dict[str, str]]): Optional mapping of column names to labels.\n\n    \"\"\"\n    # Cr\u00e9ation de la table des m\u00e9ta-donn\u00e9es\n    self.create_duckdb_metadata_table(\n        table_name=metadata_table, \n        column_labels=column_labels\n    )\n\n    # Cr\u00e9ation de la table de dimensions\n    self.create_duckdb_dimension_tables(\n        table_prefix=dim_table_prefix, \n        column_labels=column_labels\n    )\n\n    # Cr\u00e9ation de la table d'informations avec les cl\u00e9s \u00e9trang\u00e8res\n    self.create_duckdb_fact_table(\n        table_name=fact_table, \n        table_prefix=dim_table_prefix, \n        column_labels=column_labels\n    )\n</code></pre>"},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.build_duckdb_schema(metadata_table)","title":"<code>metadata_table</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.build_duckdb_schema(fact_table)","title":"<code>fact_table</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.build_duckdb_schema(dim_table_prefix)","title":"<code>dim_table_prefix</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.build_duckdb_schema(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_dimension_tables","title":"create_dimension_tables","text":"<pre><code>create_dimension_tables(column_labels: Optional[Union[Dict[str, str], None]] = None) -&gt; Dict[str, DataFrame]\n</code></pre> <p>Generate dimension tables for categorical columns in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary mapping column names to labels.                             Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, DataFrame]</code> <p>A dictionary of DataFrames, where keys are column names and values are    the corresponding dimension tables.</p> Source code in <code>dashboard_template_database/builders/schema.py</code> <pre><code>def create_dimension_tables(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; Dict[str, pd.DataFrame] :\n    \"\"\"\n    Generate dimension tables for categorical columns in the dataset.\n\n    Args:\n        column_labels (dict, optional): A dictionary mapping column names to labels.\n                                        Defaults to None.\n\n    Returns:\n        dict: A dictionary of DataFrames, where keys are column names and values are \n              the corresponding dimension tables.\n    \"\"\"\n    # Cr\u00e9ation d'une table de m\u00e9ta-donn\u00e9es si cette-derni\u00e8re n'existe pas d\u00e9j\u00e0\n    if not hasattr(self, 'metadata') :\n        _ = self.create_metadata_table(column_labels=column_labels)\n\n    # Initialisation du dictionnaire des tables de dimension\n    self.dimension_tables = {}\n\n    # Parcours des tables de dimensions\n    for categorical_dimension in self.df_metadata.loc[self.df_metadata['is_categorical'], 'name'] :\n        # Extraction des modalit\u00e9s\n        self.dimension_tables[categorical_dimension] = pd.Series(self.df[categorical_dimension].unique(), name='label').to_frame().reset_index(names='value').sort_values(by='label', ascending=True, ignore_index=True)\n        # Logging\n        self.logger.info(f\"Successfully built dimension table for '{categorical_dimension}'\")\n\n    # Logging\n    self.logger.info(\"Successfully built dimension tables\")\n\n    return self.dimension_tables\n</code></pre>"},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_dimension_tables(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_duckdb_dimension_tables","title":"create_duckdb_dimension_tables","text":"<pre><code>create_duckdb_dimension_tables(table_prefix: Optional[str] = 'dim_', column_labels: Optional[Dict[str, str]] = None) -&gt; None\n</code></pre> <p>Create dimension tables in DuckDB for categorical variables.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[str]</code> <p>Prefix for dimension table names. Defaults to 'dim_'.</p> <code>'dim_'</code> <code>Optional[Dict[str, str]]</code> <p>Optional mapping of column names to labels.</p> <code>None</code> Source code in <code>dashboard_template_database/builders/tables.py</code> <pre><code>def create_duckdb_dimension_tables(self, table_prefix: Optional[str] = 'dim_', column_labels: Optional[Dict[str, str]] = None) -&gt; None:\n    \"\"\"\n    Create dimension tables in DuckDB for categorical variables.\n\n    Args:\n        table_prefix (Optional[str]): Prefix for dimension table names. Defaults to 'dim_'.\n        column_labels (Optional[Dict[str, str]]): Optional mapping of column names to labels.\n\n    \"\"\"\n    # Cr\u00e9ation du dictionnaire des tables de dimensions si elles n'existent pas d\u00e9j\u00e0\n    if not hasattr(self, 'dimension_tables'):\n        _ = self.create_dimension_tables(column_labels)\n\n    # Cr\u00e9ation de chaque table de dimension dans DuckDB\n    for dim_name, dim_df in self.dimension_tables.items():\n        # Initialisation du nom de la table\n        table_name = f\"{table_prefix}{dim_name}\"\n        # Enregistrement d'une vue temporaire\n        self.conn.register('temp_dim', dim_df)\n\n        # Cr\u00e9ation d'une table avec \"value\" comme cl\u00e9 primaire\n        self.conn.execute(f\"\"\"\n            CREATE TABLE {table_name} AS \n            SELECT\n                value,\n                label \n            FROM temp_dim\n        \"\"\")\n\n        # Ajout de la vue correspondante\n        self.conn.execute('DROP VIEW temp_dim')\n\n        # Logging\n        self.logger.info(f\"Successfully registered duckdb dimension table for {dim_name}\")\n</code></pre>"},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_duckdb_dimension_tables(table_prefix)","title":"<code>table_prefix</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_duckdb_dimension_tables(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_duckdb_fact_table","title":"create_duckdb_fact_table","text":"<pre><code>create_duckdb_fact_table(table_name: Optional[str] = 'fact_table', table_prefix: Optional[str] = 'dim_', column_labels: Optional[Dict[str, str]] = None) -&gt; None\n</code></pre> <p>Create a fact table in DuckDB with foreign key relationships to dimension tables.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[str]</code> <p>Name of the fact table in DuckDB. Defaults to 'fact_table'.</p> <code>'fact_table'</code> <code>Optional[str]</code> <p>Prefix for dimension table names. Defaults to 'dim_'.</p> <code>'dim_'</code> <code>Optional[Dict[str, str]]</code> <p>Optional mapping of column names to labels.</p> <code>None</code> Source code in <code>dashboard_template_database/builders/tables.py</code> <pre><code>def create_duckdb_fact_table(self, table_name: Optional[str] = 'fact_table', table_prefix: Optional[str] = 'dim_', column_labels: Optional[Dict[str, str]] = None) -&gt; None:\n    \"\"\"\n    Create a fact table in DuckDB with foreign key relationships to dimension tables.\n\n    Args:\n        table_name (Optional[str]): Name of the fact table in DuckDB. Defaults to 'fact_table'.\n        table_prefix (Optional[str]): Prefix for dimension table names. Defaults to 'dim_'.\n        column_labels (Optional[Dict[str, str]]): Optional mapping of column names to labels.\n\n    \"\"\"\n    # Cr\u00e9ation de la table d'informations si elle n'existe pas d\u00e9j\u00e0\n    if not hasattr(self, 'df_fact'):\n        _ = self.create_fact_table(column_labels)\n\n    # Enregistrement de la table comme vue temporaire\n    self.conn.register('temp_fact', self.df_fact)\n\n    # Initialisation de la liste des conditions de jointure avec les tables de dimensions\n    join_conditions = []\n    # Initialisation de la liste des colonnes \u00e0 s\u00e9lectionner\n    # select_columns = []\n\n    # Parcours des tables de dimensions correspondant \u00e0 des cl\u00e9s \u00e9trang\u00e8res dans la table d'informations\n    for dim_name in self.dimension_tables.keys():\n        # Initialisation du nom de la table\n        dim_table = f\"{table_prefix}{dim_name}\"\n\n        # Ajout des conditions de jointure sur la base de la colonne \"value\" des tables de dimensions\n        join_conditions.append(\n            f\"LEFT JOIN {dim_table} ON temp_fact.{dim_name} = {dim_table}.value\"\n        )\n\n        # Logging\n        self.logger.info(f\"Successfully created foreign key for dimension '{dim_name}'\")\n        # Ajout de la colonne qui correspond \u00e0 une cl\u00e9 \u00e9trang\u00e8re\n        # select_columns.append(f\"temp_fact.{dim_name}\")\n\n    # Ajout des colonnes des variables non cat\u00e9gorielles\n    # select_columns.extend([f\"temp_fact.{col}\" for col in self.df_fact.columns if col not in self.dimension_tables.keys()])\n\n    # Cr\u00e9ation de la table d'information\n    query = f\"\"\"\n    CREATE TABLE {table_name} AS \n    SELECT *\n    FROM temp_fact\n    {' '.join(join_conditions)}\n    \"\"\"\n\n    self.conn.execute(query)\n\n    # Ajout de la vue\n    self.conn.execute('DROP VIEW temp_fact')\n\n    # Logging\n    self.logger.info(f\"Successfully registered duckdb fact table\")\n</code></pre>"},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_duckdb_fact_table(table_name)","title":"<code>table_name</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_duckdb_fact_table(table_prefix)","title":"<code>table_prefix</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_duckdb_fact_table(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_duckdb_metadata_table","title":"create_duckdb_metadata_table","text":"<pre><code>create_duckdb_metadata_table(table_name: Optional[str] = 'metadata', column_labels: Optional[Dict[str, str]] = None) -&gt; None\n</code></pre> <p>Create a metadata table in DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>Optional[str]</code> <p>Name of the metadata table in DuckDB. Defaults to 'metadata'.</p> <code>'metadata'</code> <code>Optional[Dict[str, str]]</code> <p>Optional mapping of column names to labels.</p> <code>None</code> Source code in <code>dashboard_template_database/builders/tables.py</code> <pre><code>def create_duckdb_metadata_table(self, table_name: Optional[str] = 'metadata', column_labels: Optional[Dict[str, str]] = None) -&gt; None:\n    \"\"\"\n    Create a metadata table in DuckDB.\n\n    Args:\n        table_name (Optional[str]): Name of the metadata table in DuckDB. Defaults to 'metadata'.\n        column_labels (Optional[Dict[str, str]]): Optional mapping of column names to labels.\n\n    \"\"\"\n    # Cr\u00e9ation de la table des m\u00e9ta-donn\u00e9es si elle n'existe pas d\u00e9j\u00e0\n    if not hasattr(self, 'df_metadata'):\n        _ = self.create_metadata_table(column_labels)\n\n    # Conversion DataFrame en table DuckDB\n    self.conn.register('temp_metadata', self.df_metadata)\n    self.conn.execute(f\"\"\"\n        CREATE TABLE {table_name} AS \n        SELECT * FROM temp_metadata\n    \"\"\")\n    self.conn.execute('DROP VIEW temp_metadata')\n\n    # Logging\n    self.logger.info(\"Successfully registered duckdb meta-data table\")\n</code></pre>"},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_duckdb_metadata_table(table_name)","title":"<code>table_name</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_duckdb_metadata_table(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_fact_table","title":"create_fact_table","text":"<pre><code>create_fact_table(column_labels: Optional[Union[Dict[str, str], None]] = None) -&gt; DataFrame\n</code></pre> <p>Generate a fact table by replacing categorical values with corresponding IDs.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary mapping column names to labels.                             Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The fact table with categorical values replaced by IDs.</p> Source code in <code>dashboard_template_database/builders/schema.py</code> <pre><code>def create_fact_table(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a fact table by replacing categorical values with corresponding IDs.\n\n    Args:\n        column_labels (dict, optional): A dictionary mapping column names to labels.\n                                        Defaults to None.\n\n    Returns:\n        pd.DataFrame: The fact table with categorical values replaced by IDs.\n    \"\"\"\n    # Cr\u00e9ation des tables de dimensions si ces-derni\u00e8res n'existent pas\n    if not hasattr(self, 'dimension_tables') :\n        _ = self.create_dimension_tables(column_labels=column_labels)\n\n    # Initialisation de la table des informations\n    self.df_fact = self.df.copy()\n\n    # Remplacement des labels par leur valeur\n    for column in self.dimension_tables.keys() :\n        # Construction du dictionnaire de passage\n        dict_label_value = {d['label'] : d['value'] for d in self.dimension_tables[column].to_dict(orient='records')}\n        # Remplacement des valeurs\n        self.df_fact[column] = self.df_fact[column].replace(dict_label_value)\n        # Logging\n        self.logger.info(f\"Successfully replace modalities by ids in column '{column}'\")\n\n    # Logging\n    self.logger.info(\"Successfully built fact table\")\n\n    return self.df_fact\n</code></pre>"},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_fact_table(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_metadata_table","title":"create_metadata_table","text":"<pre><code>create_metadata_table(column_labels: Optional[Union[Dict[str, str], None]] = None) -&gt; Dict\n</code></pre> <p>Automatically infer metadata for the DataFrame's columns, including types, labels,  and categorical attributes.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary mapping column names to labels.                             Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>pd.DataFrame: A DataFrame containing metadata for each column in the input dataset.</p> Source code in <code>dashboard_template_database/builders/schema.py</code> <pre><code>def create_metadata_table(self, column_labels : Optional[Union[Dict[str, str], None]]= None) -&gt; Dict:\n    \"\"\"\n    Automatically infer metadata for the DataFrame's columns, including types, labels, \n    and categorical attributes.\n\n    Args:\n        column_labels (dict, optional): A dictionary mapping column names to labels.\n                                        Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing metadata for each column in the input dataset.\n    \"\"\"\n    # Initialisation de la liste des m\u00e9ta-donn\u00e9es\n    list_metadata = []\n    # Parcours des colonnes du jeu de donn\u00e9es\n    for col in self.df.columns:\n        # Extraction du type de la colonne\n        dtype = str(self.df[col].dtype)\n        # Initialisation des m\u00e9ta-donn\u00e9es associ\u00e9es \u00e0 la colonne\n        if column_labels is not None :\n            metadata = {\n                'name': col,\n                'label': column_labels[col] if col in column_labels.keys() else col.replace('_', ' ').title(),\n                'python_type': dtype,\n                'sql_type': self._map_python_to_sql_type(dtype),\n                'is_categorical': False,\n                # 'modalities': None\n            }\n        else :\n            metadata = {\n                'name': col,\n                'label': col.replace('_', ' ').title(),\n                'python_type': dtype,\n                'sql_type': self._map_python_to_sql_type(dtype),\n                'is_categorical': False,\n                # 'modalities': None\n            }\n\n        # Logging\n        self.logger.info(f\"Successfully extracted meta-data from column '{col}'\")\n\n        # Si la colonne est object\n        if dtype == 'object' :\n            # Calcul du nombre de modalit\u00e9s\n            n_modalities = self.df[col].nunique()\n            # Si le nombre de modalit\u00e9s dans la colonne est inf\u00e9rieur au seuil, la variable est cat\u00e9gorielle\n            if  n_modalities &lt;= self.categorical_threshold:\n                # Mise \u00e0 jour du type de la variable\n                metadata['is_categorical'] = True\n                # Logging\n                self.logger.info(f\"The column '{col}' is of type 'object' and the number of modalities {n_modalities} satisfies the categorical threshold criteria {self.categorical_threshold}\")\n                # Mise \u00e0 jour des modalit\u00e9s\n                # metadata['modalities'] = str(self.df[col].dropna().unique().tolist())\n            else :\n                # Logging\n                self.logger.warning(f\"The column '{col}' is  of type 'object' but the number of modalities {n_modalities} exceeds the categorical threshold criteria {self.categorical_threshold}\")\n\n        # Ajout au dictionnaire\n        list_metadata.append(metadata)            \n\n    # Cr\u00e9ation d'un DataFrame\n    self.df_metadata = pd.DataFrame.from_dict(list_metadata).sort_values(by='label', ascending=True, ignore_index=True)\n\n    # Logging\n    self.logger.info(\"Successfully built the meta-data DataFrame\")\n\n    return self.df_metadata\n</code></pre>"},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.create_metadata_table(column_labels)","title":"<code>column_labels</code>","text":""},{"location":"api/builders/tables/DuckdbTablesBuilder/#dashboard_template_database.builders.tables.DuckdbTablesBuilder.display_schema","title":"display_schema","text":"<pre><code>display_schema() -&gt; None\n</code></pre> <p>Display the structure of all tables in the DuckDB schema.</p> Source code in <code>dashboard_template_database/builders/tables.py</code> <pre><code>def display_schema(self) -&gt; None:\n    \"\"\"\n    Display the structure of all tables in the DuckDB schema.\n    \"\"\"\n    # Extraction des tables\n    tables = self.conn.execute(\"SHOW TABLES\").fetchall()\n    # Logging\n    self.logger.info(\"\\n Created Tables:\")\n    # Parcours des tables\n    for table in tables:\n        # Affichage de la structure\n        self.logger.info(f\"\\n {table[0]} Structure:\")\n        # Extraction des informations relatives \u00e0 la table\n        table_info = self.conn.execute(f\"DESCRIBE {table[0]}\").fetchall()\n        # Affichage de chaque information\n        for col in table_info:\n            self.logger.info(f\"  {col[0]}: {col[1]}\")\n</code></pre>"},{"location":"api/storage/Loader/","title":"Loader","text":""},{"location":"api/storage/Loader/#dashboard_template_database.storage.loader.Loader","title":"Loader","text":"<p>A unified class for loading data from both S3 and local storage.</p> <p>This class provides functionality to load data from either Amazon S3 or local storage based on whether a bucket name is specified. It inherits from S3Loader to handle S3-specific operations.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The package to use for S3 connections ('s3fs' or 'boto3'). Defaults to \"boto3\".</p> <code>'boto3'</code> <p>Attributes:</p> Name Type Description <code>s3</code> <p>The S3 connection object, initialized when needed.</p> <p>Examples:</p> <p>Load data from S3:</p> <pre><code>&gt;&gt;&gt; loader = Loader(s3_package='boto3')\n&gt;&gt;&gt; s3_data = loader.load(\n...     filepath='data/sales.csv',\n...     bucket='my-bucket',\n...     aws_access_key_id='YOUR_KEY',\n...     aws_secret_access_key='YOUR_SECRET'\n... )\n</code></pre> <p>Load data from local storage:</p> <pre><code>&gt;&gt;&gt; loader = Loader()\n&gt;&gt;&gt; local_data = loader.load(filepath='data/sales.csv')\n</code></pre> Notes <ul> <li>When loading from S3, AWS credentials can be provided either through   environment variables or as parameters.</li> <li>For local loading, all standard formats are supported: CSV, Excel, JSON,   Pickle, GeoJSON, and Parquet.</li> </ul> <p>Methods:</p> Name Description <code>connect</code> <p>Establish a connection to the S3 bucket.</p> <code>load</code> <p>Load data from either S3 or local storage.</p> Source code in <code>dashboard_template_database/storage/loader.py</code> <pre><code>class Loader(S3Loader):\n    \"\"\"A unified class for loading data from both S3 and local storage.\n\n    This class provides functionality to load data from either Amazon S3 or local storage\n    based on whether a bucket name is specified. It inherits from S3Loader to handle\n    S3-specific operations.\n\n    Args:\n        s3_package (str, optional): The package to use for S3 connections ('s3fs' or 'boto3').\n            Defaults to \"boto3\".\n\n    Attributes:\n        s3: The S3 connection object, initialized when needed.\n\n    Examples:\n        Load data from S3:\n        &gt;&gt;&gt; loader = Loader(s3_package='boto3')\n        &gt;&gt;&gt; s3_data = loader.load(\n        ...     filepath='data/sales.csv',\n        ...     bucket='my-bucket',\n        ...     aws_access_key_id='YOUR_KEY',\n        ...     aws_secret_access_key='YOUR_SECRET'\n        ... )\n\n        Load data from local storage:\n        &gt;&gt;&gt; loader = Loader()\n        &gt;&gt;&gt; local_data = loader.load(filepath='data/sales.csv')\n\n    Notes:\n        - When loading from S3, AWS credentials can be provided either through\n          environment variables or as parameters.\n        - For local loading, all standard formats are supported: CSV, Excel, JSON,\n          Pickle, GeoJSON, and Parquet.\n    \"\"\"\n\n    def __init__(self, s3_package: Optional[str] = \"boto3\") -&gt; None:\n        \"\"\"Initialize the Loader with specified S3 package.\n\n        Args:\n            s3_package (str, optional): Package to use for S3 connections.\n                Must be either 's3fs' or 'boto3'. Defaults to \"boto3\".\n        \"\"\"\n        super().__init__(s3_package=s3_package)\n\n    def load(self, filepath: str, bucket: Optional[str] = None, **kwargs) -&gt; Any:\n        \"\"\"Load data from either S3 or local storage.\n\n        Args:\n            filepath (str): Path to the file. For S3, this is the key within the bucket.\n                For local storage, this is the path on the filesystem.\n            bucket (str, optional): S3 bucket name. If None, loads from local storage.\n            **kwargs: Additional arguments passed to the underlying loader:\n                - For S3: aws_access_key_id, aws_secret_access_key, aws_session_token,\n                  endpoint_url, verify\n                - For both: file format specific options (encoding, separator, etc.)\n\n        Returns:\n            Any: The loaded data in appropriate format based on file extension:\n                - .csv, .xlsx, .xls -&gt; pandas DataFrame\n                - .json -&gt; dict or pandas DataFrame\n                - .pkl -&gt; pickled object\n                - .geojson -&gt; GeoDataFrame\n                - .parquet -&gt; pandas DataFrame\n\n        Raises:\n            ValueError: If the file extension is not supported\n            FileNotFoundError: If the local file doesn't exist\n            botocore.exceptions.ClientError: If there are S3 access issues\n\n        Examples:\n            Load CSV from S3:\n            &gt;&gt;&gt; data = loader.load(\n            ...     filepath='data.csv',\n            ...     bucket='my-bucket',\n            ...     aws_access_key_id='KEY',\n            ...     aws_secret_access_key='SECRET'\n            ... )\n\n            Load local Excel file with specific options:\n            &gt;&gt;&gt; data = loader.load(\n            ...     filepath='data.xlsx',\n            ...     sheet_name='Sales',\n            ...     skiprows=1\n            ... )\n        \"\"\"\n        if bucket is not None:\n            # Extract S3-specific kwargs\n            s3_kwargs = {\n                k: kwargs.pop(k)\n                for k in [\n                    \"aws_access_key_id\",\n                    \"aws_secret_access_key\",\n                    \"aws_session_token\",\n                    \"endpoint_url\",\n                    \"verify\",\n                ]\n                if k in kwargs\n            }\n            # Connect if needed\n            if not hasattr(self, \"s3\"):\n                self.connect(**s3_kwargs)\n            # Use parent S3Loader's load method\n            return super().load(bucket=bucket, key=filepath, **kwargs)\n        else:\n            # Use parent LocalLoader's load_local method\n            return load_local(filepath=filepath, **kwargs)\n</code></pre>"},{"location":"api/storage/Loader/#dashboard_template_database.storage.loader.Loader(s3_package)","title":"<code>s3_package</code>","text":""},{"location":"api/storage/Loader/#dashboard_template_database.storage.loader.Loader.connect","title":"connect","text":"<pre><code>connect(**kwargs) -&gt; None\n</code></pre> <p>Establish a connection to the S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <p>Additional keyword arguments for establishing the connection.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>object</code> <code>None</code> <p>The established S3 connection.</p> <p>Example :</p> <p>s3_loader = S3Loader(package='boto3') s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')</p> Source code in <code>dashboard_template_database/storage/s3/loader.py</code> <pre><code>def connect(self, **kwargs) -&gt; None:\n    \"\"\"\n    Establish a connection to the S3 bucket.\n\n    Args:\n        **kwargs: Additional keyword arguments for establishing the connection.\n\n    Returns:\n        object: The established S3 connection.\n\n    Example :\n    &gt;&gt;&gt; s3_loader = S3Loader(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n    \"\"\"\n    # Etablissement d'une connection\n    return self._connect(**kwargs)\n</code></pre>"},{"location":"api/storage/Loader/#dashboard_template_database.storage.loader.Loader.connect(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/storage/Loader/#dashboard_template_database.storage.loader.Loader.load","title":"load","text":"<pre><code>load(filepath: str, bucket: Optional[str] = None, **kwargs) -&gt; Any\n</code></pre> <p>Load data from either S3 or local storage.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Path to the file. For S3, this is the key within the bucket. For local storage, this is the path on the filesystem.</p> required <code>str</code> <p>S3 bucket name. If None, loads from local storage.</p> <code>None</code> <p>Additional arguments passed to the underlying loader: - For S3: aws_access_key_id, aws_secret_access_key, aws_session_token,   endpoint_url, verify - For both: file format specific options (encoding, separator, etc.)</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The loaded data in appropriate format based on file extension: - .csv, .xlsx, .xls -&gt; pandas DataFrame - .json -&gt; dict or pandas DataFrame - .pkl -&gt; pickled object - .geojson -&gt; GeoDataFrame - .parquet -&gt; pandas DataFrame</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file extension is not supported</p> <code>FileNotFoundError</code> <p>If the local file doesn't exist</p> <code>ClientError</code> <p>If there are S3 access issues</p> <p>Examples:</p> <p>Load CSV from S3:</p> <pre><code>&gt;&gt;&gt; data = loader.load(\n...     filepath='data.csv',\n...     bucket='my-bucket',\n...     aws_access_key_id='KEY',\n...     aws_secret_access_key='SECRET'\n... )\n</code></pre> <p>Load local Excel file with specific options:</p> <pre><code>&gt;&gt;&gt; data = loader.load(\n...     filepath='data.xlsx',\n...     sheet_name='Sales',\n...     skiprows=1\n... )\n</code></pre> Source code in <code>dashboard_template_database/storage/loader.py</code> <pre><code>def load(self, filepath: str, bucket: Optional[str] = None, **kwargs) -&gt; Any:\n    \"\"\"Load data from either S3 or local storage.\n\n    Args:\n        filepath (str): Path to the file. For S3, this is the key within the bucket.\n            For local storage, this is the path on the filesystem.\n        bucket (str, optional): S3 bucket name. If None, loads from local storage.\n        **kwargs: Additional arguments passed to the underlying loader:\n            - For S3: aws_access_key_id, aws_secret_access_key, aws_session_token,\n              endpoint_url, verify\n            - For both: file format specific options (encoding, separator, etc.)\n\n    Returns:\n        Any: The loaded data in appropriate format based on file extension:\n            - .csv, .xlsx, .xls -&gt; pandas DataFrame\n            - .json -&gt; dict or pandas DataFrame\n            - .pkl -&gt; pickled object\n            - .geojson -&gt; GeoDataFrame\n            - .parquet -&gt; pandas DataFrame\n\n    Raises:\n        ValueError: If the file extension is not supported\n        FileNotFoundError: If the local file doesn't exist\n        botocore.exceptions.ClientError: If there are S3 access issues\n\n    Examples:\n        Load CSV from S3:\n        &gt;&gt;&gt; data = loader.load(\n        ...     filepath='data.csv',\n        ...     bucket='my-bucket',\n        ...     aws_access_key_id='KEY',\n        ...     aws_secret_access_key='SECRET'\n        ... )\n\n        Load local Excel file with specific options:\n        &gt;&gt;&gt; data = loader.load(\n        ...     filepath='data.xlsx',\n        ...     sheet_name='Sales',\n        ...     skiprows=1\n        ... )\n    \"\"\"\n    if bucket is not None:\n        # Extract S3-specific kwargs\n        s3_kwargs = {\n            k: kwargs.pop(k)\n            for k in [\n                \"aws_access_key_id\",\n                \"aws_secret_access_key\",\n                \"aws_session_token\",\n                \"endpoint_url\",\n                \"verify\",\n            ]\n            if k in kwargs\n        }\n        # Connect if needed\n        if not hasattr(self, \"s3\"):\n            self.connect(**s3_kwargs)\n        # Use parent S3Loader's load method\n        return super().load(bucket=bucket, key=filepath, **kwargs)\n    else:\n        # Use parent LocalLoader's load_local method\n        return load_local(filepath=filepath, **kwargs)\n</code></pre>"},{"location":"api/storage/Loader/#dashboard_template_database.storage.loader.Loader.load(filepath)","title":"<code>filepath</code>","text":""},{"location":"api/storage/Loader/#dashboard_template_database.storage.loader.Loader.load(bucket)","title":"<code>bucket</code>","text":""},{"location":"api/storage/Loader/#dashboard_template_database.storage.loader.Loader.load(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/storage/Saver/","title":"Saver","text":""},{"location":"api/storage/Saver/#dashboard_template_database.storage.saver.Saver","title":"Saver","text":"<p>A unified class for saving data to both S3 and local storage.</p> <p>This class provides functionality to save data to either Amazon S3 or local storage based on whether a bucket name is specified. It inherits from S3Saver to handle S3-specific operations.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The package to use for S3 connections ('s3fs' or 'boto3'). Defaults to \"boto3\".</p> <code>'boto3'</code> <p>Attributes:</p> Name Type Description <code>s3</code> <p>The S3 connection object, initialized when needed.</p> <p>Examples:</p> <p>Save DataFrame to S3:</p> <pre><code>&gt;&gt;&gt; saver = Saver(s3_package='boto3')\n&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n&gt;&gt;&gt; saver.save(\n...     filepath='data/output.csv',\n...     bucket='my-bucket',\n...     obj=df,\n...     aws_access_key_id='YOUR_KEY',\n...     aws_secret_access_key='YOUR_SECRET'\n... )\n</code></pre> <p>Save DataFrame locally:</p> <pre><code>&gt;&gt;&gt; saver = Saver()\n&gt;&gt;&gt; saver.save(filepath='data/output.csv', obj=df)\n</code></pre> <p>Save multiple DataFrames to Excel sheets:</p> <pre><code>&gt;&gt;&gt; sheets = {\n...     'Sheet1': df1,\n...     'Sheet2': df2\n... }\n&gt;&gt;&gt; saver.save(filepath='output.xlsx', obj=sheets)\n</code></pre> <p>Methods:</p> Name Description <code>connect</code> <p>Establish a connection to the S3 bucket.</p> <code>save</code> <p>Save data to either S3 or local storage.</p> Source code in <code>dashboard_template_database/storage/saver.py</code> <pre><code>class Saver(S3Saver):\n    \"\"\"A unified class for saving data to both S3 and local storage.\n\n    This class provides functionality to save data to either Amazon S3 or local storage\n    based on whether a bucket name is specified. It inherits from S3Saver to handle\n    S3-specific operations.\n\n    Args:\n        s3_package (str, optional): The package to use for S3 connections ('s3fs' or 'boto3').\n            Defaults to \"boto3\".\n\n    Attributes:\n        s3: The S3 connection object, initialized when needed.\n\n    Examples:\n        Save DataFrame to S3:\n        &gt;&gt;&gt; saver = Saver(s3_package='boto3')\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n        &gt;&gt;&gt; saver.save(\n        ...     filepath='data/output.csv',\n        ...     bucket='my-bucket',\n        ...     obj=df,\n        ...     aws_access_key_id='YOUR_KEY',\n        ...     aws_secret_access_key='YOUR_SECRET'\n        ... )\n\n        Save DataFrame locally:\n        &gt;&gt;&gt; saver = Saver()\n        &gt;&gt;&gt; saver.save(filepath='data/output.csv', obj=df)\n\n        Save multiple DataFrames to Excel sheets:\n        &gt;&gt;&gt; sheets = {\n        ...     'Sheet1': df1,\n        ...     'Sheet2': df2\n        ... }\n        &gt;&gt;&gt; saver.save(filepath='output.xlsx', obj=sheets)\n    \"\"\"\n\n    def __init__(self, s3_package: Optional[str] = \"boto3\"):\n        \"\"\"Initialize the Saver with specified S3 package.\n\n        Args:\n            s3_package (str, optional): Package to use for S3 connections.\n                Must be either 's3fs' or 'boto3'. Defaults to \"boto3\".\n        \"\"\"\n        super().__init__(s3_package=s3_package)\n\n    def save(\n        self,\n        filepath: str,\n        obj: Optional[object] = None,\n        bucket: Optional[str] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Save data to either S3 or local storage.\n\n        Args:\n            filepath (str): Path for saving the file. For S3, this is the key within\n                the bucket. For local storage, this is the path on the filesystem.\n            obj: The object to save. Type requirements depend on the file extension:\n                - .csv, .parquet: pandas DataFrame\n                - .xlsx, .xls: pandas DataFrame or dict of DataFrames\n                - .json: Any JSON-serializable object or pandas DataFrame\n                - .pkl: Any picklable object\n                - .png: Active matplotlib figure\n                - .geojson: GeoDataFrame\n            bucket (str, optional): S3 bucket name. If None, saves to local storage.\n            **kwargs: Additional arguments for saving:\n                - For S3: aws_access_key_id, aws_secret_access_key, aws_session_token,\n                  endpoint_url, verify\n                - For both: format-specific options (index, encoding, etc.)\n\n        Raises:\n            ValueError: If the file extension is not supported\n            TypeError: If obj type doesn't match the file extension requirements\n            IOError: If there are issues writing to local storage\n            botocore.exceptions.ClientError: If there are S3 access issues\n\n        Examples:\n            Save DataFrame to CSV in S3:\n            &gt;&gt;&gt; saver.save(\n            ...     filepath='data.csv',\n            ...     bucket='my-bucket',\n            ...     obj=df,\n            ...     index=False\n            ... )\n\n            Save DataFrame to local Excel with specific options:\n            &gt;&gt;&gt; saver.save(\n            ...     filepath='data.xlsx',\n            ...     obj=df,\n            ...     sheet_name='Data',\n            ...     index=False\n            ... )\n\n            Save multiple DataFrames to Excel sheets:\n            &gt;&gt;&gt; sheets = {'Sales': sales_df, 'Costs': costs_df}\n            &gt;&gt;&gt; saver.save(\n            ...     filepath='report.xlsx',\n            ...     obj=sheets\n            ... )\n        \"\"\"\n        if bucket is not None:\n            # Extract S3-specific kwargs\n            s3_kwargs = {\n                k: kwargs.pop(k)\n                for k in [\n                    \"aws_access_key_id\",\n                    \"aws_secret_access_key\",\n                    \"aws_session_token\",\n                    \"endpoint_url\",\n                    \"verify\",\n                ]\n                if k in kwargs\n            }\n            # Connect if needed\n            if not hasattr(self, \"s3\"):\n                self.connect(**s3_kwargs)\n            # Use parent S3Saver's save method\n            super().save(bucket=bucket, key=filepath, obj=obj, **kwargs)\n        else:\n            # Use LocalSaver's save_local method\n            save_local(filepath=filepath, obj=obj, **kwargs)\n</code></pre>"},{"location":"api/storage/Saver/#dashboard_template_database.storage.saver.Saver(s3_package)","title":"<code>s3_package</code>","text":""},{"location":"api/storage/Saver/#dashboard_template_database.storage.saver.Saver.connect","title":"connect","text":"<pre><code>connect(**kwargs) -&gt; None\n</code></pre> <p>Establish a connection to the S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <p>Additional keyword arguments for establishing the connection.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>object</code> <code>None</code> <p>The established S3 connection.</p> <p>Example usage:</p> <p>s3_saver = S3Saver(package='boto3') s3_connection = s3_saver.connect(     aws_access_key_id='your_access_key',     aws_secret_access_key='your_secret_key' )</p> Source code in <code>dashboard_template_database/storage/s3/saver.py</code> <pre><code>def connect(self, **kwargs) -&gt; None:\n    \"\"\"\n    Establish a connection to the S3 bucket.\n\n    Args:\n        **kwargs: Additional keyword arguments for establishing the connection.\n\n    Returns:\n        object: The established S3 connection.\n\n    Example usage:\n    &gt;&gt;&gt; s3_saver = S3Saver(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_saver.connect(\n        aws_access_key_id='your_access_key',\n        aws_secret_access_key='your_secret_key'\n    )\n    \"\"\"\n    # Etablissement d'une connection\n    return self._connect(**kwargs)\n</code></pre>"},{"location":"api/storage/Saver/#dashboard_template_database.storage.saver.Saver.connect(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/storage/Saver/#dashboard_template_database.storage.saver.Saver.save","title":"save","text":"<pre><code>save(filepath: str, obj: Optional[object] = None, bucket: Optional[str] = None, **kwargs) -&gt; None\n</code></pre> <p>Save data to either S3 or local storage.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Path for saving the file. For S3, this is the key within the bucket. For local storage, this is the path on the filesystem.</p> required <code>Optional[object]</code> <p>The object to save. Type requirements depend on the file extension: - .csv, .parquet: pandas DataFrame - .xlsx, .xls: pandas DataFrame or dict of DataFrames - .json: Any JSON-serializable object or pandas DataFrame - .pkl: Any picklable object - .png: Active matplotlib figure - .geojson: GeoDataFrame</p> <code>None</code> <code>str</code> <p>S3 bucket name. If None, saves to local storage.</p> <code>None</code> <p>Additional arguments for saving: - For S3: aws_access_key_id, aws_secret_access_key, aws_session_token,   endpoint_url, verify - For both: format-specific options (index, encoding, etc.)</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file extension is not supported</p> <code>TypeError</code> <p>If obj type doesn't match the file extension requirements</p> <code>IOError</code> <p>If there are issues writing to local storage</p> <code>ClientError</code> <p>If there are S3 access issues</p> <p>Examples:</p> <p>Save DataFrame to CSV in S3:</p> <pre><code>&gt;&gt;&gt; saver.save(\n...     filepath='data.csv',\n...     bucket='my-bucket',\n...     obj=df,\n...     index=False\n... )\n</code></pre> <p>Save DataFrame to local Excel with specific options:</p> <pre><code>&gt;&gt;&gt; saver.save(\n...     filepath='data.xlsx',\n...     obj=df,\n...     sheet_name='Data',\n...     index=False\n... )\n</code></pre> <p>Save multiple DataFrames to Excel sheets:</p> <pre><code>&gt;&gt;&gt; sheets = {'Sales': sales_df, 'Costs': costs_df}\n&gt;&gt;&gt; saver.save(\n...     filepath='report.xlsx',\n...     obj=sheets\n... )\n</code></pre> Source code in <code>dashboard_template_database/storage/saver.py</code> <pre><code>def save(\n    self,\n    filepath: str,\n    obj: Optional[object] = None,\n    bucket: Optional[str] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Save data to either S3 or local storage.\n\n    Args:\n        filepath (str): Path for saving the file. For S3, this is the key within\n            the bucket. For local storage, this is the path on the filesystem.\n        obj: The object to save. Type requirements depend on the file extension:\n            - .csv, .parquet: pandas DataFrame\n            - .xlsx, .xls: pandas DataFrame or dict of DataFrames\n            - .json: Any JSON-serializable object or pandas DataFrame\n            - .pkl: Any picklable object\n            - .png: Active matplotlib figure\n            - .geojson: GeoDataFrame\n        bucket (str, optional): S3 bucket name. If None, saves to local storage.\n        **kwargs: Additional arguments for saving:\n            - For S3: aws_access_key_id, aws_secret_access_key, aws_session_token,\n              endpoint_url, verify\n            - For both: format-specific options (index, encoding, etc.)\n\n    Raises:\n        ValueError: If the file extension is not supported\n        TypeError: If obj type doesn't match the file extension requirements\n        IOError: If there are issues writing to local storage\n        botocore.exceptions.ClientError: If there are S3 access issues\n\n    Examples:\n        Save DataFrame to CSV in S3:\n        &gt;&gt;&gt; saver.save(\n        ...     filepath='data.csv',\n        ...     bucket='my-bucket',\n        ...     obj=df,\n        ...     index=False\n        ... )\n\n        Save DataFrame to local Excel with specific options:\n        &gt;&gt;&gt; saver.save(\n        ...     filepath='data.xlsx',\n        ...     obj=df,\n        ...     sheet_name='Data',\n        ...     index=False\n        ... )\n\n        Save multiple DataFrames to Excel sheets:\n        &gt;&gt;&gt; sheets = {'Sales': sales_df, 'Costs': costs_df}\n        &gt;&gt;&gt; saver.save(\n        ...     filepath='report.xlsx',\n        ...     obj=sheets\n        ... )\n    \"\"\"\n    if bucket is not None:\n        # Extract S3-specific kwargs\n        s3_kwargs = {\n            k: kwargs.pop(k)\n            for k in [\n                \"aws_access_key_id\",\n                \"aws_secret_access_key\",\n                \"aws_session_token\",\n                \"endpoint_url\",\n                \"verify\",\n            ]\n            if k in kwargs\n        }\n        # Connect if needed\n        if not hasattr(self, \"s3\"):\n            self.connect(**s3_kwargs)\n        # Use parent S3Saver's save method\n        super().save(bucket=bucket, key=filepath, obj=obj, **kwargs)\n    else:\n        # Use LocalSaver's save_local method\n        save_local(filepath=filepath, obj=obj, **kwargs)\n</code></pre>"},{"location":"api/storage/Saver/#dashboard_template_database.storage.saver.Saver.save(filepath)","title":"<code>filepath</code>","text":""},{"location":"api/storage/Saver/#dashboard_template_database.storage.saver.Saver.save(obj)","title":"<code>obj</code>","text":""},{"location":"api/storage/Saver/#dashboard_template_database.storage.saver.Saver.save(bucket)","title":"<code>bucket</code>","text":""},{"location":"api/storage/Saver/#dashboard_template_database.storage.saver.Saver.save(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/storage/local/load_local/","title":"load_local","text":""},{"location":"api/storage/local/load_local/#dashboard_template_database.storage.local.loader.load_local","title":"load_local","text":"<pre><code>load_local(filepath: str, **kwargs) -&gt; Any\n</code></pre> <p>Load data from a local file based on its extension.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Path to the local file</p> required <p>Additional arguments for reading the file: - CSV: encoding, separator, etc. - Excel: sheet_name, skiprows, etc. - JSON: encoding, etc. - Others: format-specific options</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The loaded data in appropriate format: - .xlsx, .xls -&gt; pandas DataFrame - .csv -&gt; pandas DataFrame - .json -&gt; dict or pandas DataFrame - .pkl -&gt; pickled object - .geojson -&gt; GeoDataFrame - .parquet -&gt; pandas DataFrame</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file extension is not supported</p> <code>FileNotFoundError</code> <p>If the file doesn't exist</p> <code>EmptyDataError</code> <p>If the file is empty</p> <p>Examples:</p> <p>Load CSV with specific options:</p> <pre><code>&gt;&gt;&gt; data = load_local(\n...     'data.csv',\n...     encoding='utf-8',\n...     sep=';'\n... )\n</code></pre> <p>Load specific Excel sheet:</p> <pre><code>&gt;&gt;&gt; data = load_local(\n...     'report.xlsx',\n...     sheet_name='Sales',\n...     skiprows=1\n... )\n</code></pre> <p>Load GeoJSON:</p> <pre><code>&gt;&gt;&gt; geodata = load_local('map.geojson')\n</code></pre> Source code in <code>dashboard_template_database/storage/local/loader.py</code> <pre><code>def load_local(filepath: str, **kwargs) -&gt; Any:\n    \"\"\"Load data from a local file based on its extension.\n\n    Args:\n        filepath (str): Path to the local file\n        **kwargs: Additional arguments for reading the file:\n            - CSV: encoding, separator, etc.\n            - Excel: sheet_name, skiprows, etc.\n            - JSON: encoding, etc.\n            - Others: format-specific options\n\n    Returns:\n        Any: The loaded data in appropriate format:\n            - .xlsx, .xls -&gt; pandas DataFrame\n            - .csv -&gt; pandas DataFrame\n            - .json -&gt; dict or pandas DataFrame\n            - .pkl -&gt; pickled object\n            - .geojson -&gt; GeoDataFrame\n            - .parquet -&gt; pandas DataFrame\n\n    Raises:\n        ValueError: If the file extension is not supported\n        FileNotFoundError: If the file doesn't exist\n        pd.errors.EmptyDataError: If the file is empty\n\n    Examples:\n        Load CSV with specific options:\n        &gt;&gt;&gt; data = load_local(\n        ...     'data.csv',\n        ...     encoding='utf-8',\n        ...     sep=';'\n        ... )\n\n        Load specific Excel sheet:\n        &gt;&gt;&gt; data = load_local(\n        ...     'report.xlsx',\n        ...     sheet_name='Sales',\n        ...     skiprows=1\n        ... )\n\n        Load GeoJSON:\n        &gt;&gt;&gt; geodata = load_local('map.geojson')\n    \"\"\"\n    # Convert string path to Path object for better handling\n    path = Path(filepath)\n    extension = path.suffix.lower()[1:]  # Remove the dot and convert to lowercase\n\n    # Handle different file types\n    if extension == \"xlsx\":\n        return pd.read_excel(filepath, engine=\"openpyxl\", **kwargs)\n    elif extension == \"xls\":\n        return pd.read_excel(filepath, engine=\"xlrd\", **kwargs)\n    elif extension == \"csv\":\n        return pd.read_csv(filepath, **kwargs)\n    elif extension == \"json\":\n        with open(filepath, \"r\") as f:\n            return json.load(f, **kwargs)\n    elif extension == \"pkl\":\n        return pd.read_pickle(filepath, **kwargs)\n    elif extension == \"geojson\":\n        return read_file(filepath, **kwargs)\n    elif extension == \"parquet\":\n        return pd.read_parquet(filepath, **kwargs)\n    else:\n        raise ValueError(\n            \"Invalid extension: should be in ['xlsx', 'xls', 'json', 'pkl', 'geojson', 'csv', 'parquet'].\"\n        )\n</code></pre>"},{"location":"api/storage/local/load_local/#dashboard_template_database.storage.local.loader.load_local(filepath)","title":"<code>filepath</code>","text":""},{"location":"api/storage/local/load_local/#dashboard_template_database.storage.local.loader.load_local(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/storage/local/save_local/","title":"save_local","text":""},{"location":"api/storage/local/save_local/#dashboard_template_database.storage.local.saver.save_local","title":"save_local","text":"<pre><code>save_local(filepath: str, obj: Optional[object] = None, **kwargs) -&gt; None\n</code></pre> <p>Save an object to a local file based on its extension.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Path where to save the file</p> required <code>Optional[object]</code> <p>The object to save. Type requirements depend on the file extension: - .csv, .parquet: pandas DataFrame - .xlsx, .xls: pandas DataFrame or dict of DataFrames - .json: Any JSON-serializable object or pandas DataFrame - .pkl: Any picklable object - .png: Active matplotlib figure - .geojson: GeoDataFrame</p> <code>None</code> <p>Additional arguments for saving: - CSV: index, encoding, etc. - Excel: sheet_name, index, etc. - JSON: indent, orient, etc. - Others: format-specific options</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file extension is not supported</p> <code>TypeError</code> <p>If obj type doesn't match the file extension requirements</p> <code>IOError</code> <p>If there are issues writing to the file</p> <code>OSError</code> <p>If the directory structure doesn't exist</p> <p>Examples:</p> <p>Save DataFrame to CSV:</p> <pre><code>&gt;&gt;&gt; save_local(\n...     'data.csv',\n...     df,\n...     index=False,\n...     encoding='utf-8'\n... )\n</code></pre> <p>Save multiple DataFrames to Excel sheets:</p> <pre><code>&gt;&gt;&gt; sheets = {\n...     'Sales': sales_df,\n...     'Costs': costs_df\n... }\n&gt;&gt;&gt; save_local('report.xlsx', sheets)\n</code></pre> <p>Save matplotlib figure:</p> <pre><code>&gt;&gt;&gt; plt.plot([1, 2, 3])\n&gt;&gt;&gt; save_local('plot.png', dpi=300)\n</code></pre> Notes <ul> <li>Parent directories will be created if they don't exist</li> <li>For Excel files with multiple sheets, sheet names are truncated to 31 chars</li> </ul> Source code in <code>dashboard_template_database/storage/local/saver.py</code> <pre><code>def save_local(filepath: str, obj: Optional[object] = None, **kwargs) -&gt; None:\n    \"\"\"Save an object to a local file based on its extension.\n\n    Args:\n        filepath (str): Path where to save the file\n        obj: The object to save. Type requirements depend on the file extension:\n            - .csv, .parquet: pandas DataFrame\n            - .xlsx, .xls: pandas DataFrame or dict of DataFrames\n            - .json: Any JSON-serializable object or pandas DataFrame\n            - .pkl: Any picklable object\n            - .png: Active matplotlib figure\n            - .geojson: GeoDataFrame\n        **kwargs: Additional arguments for saving:\n            - CSV: index, encoding, etc.\n            - Excel: sheet_name, index, etc.\n            - JSON: indent, orient, etc.\n            - Others: format-specific options\n\n    Raises:\n        ValueError: If the file extension is not supported\n        TypeError: If obj type doesn't match the file extension requirements\n        IOError: If there are issues writing to the file\n        OSError: If the directory structure doesn't exist\n\n    Examples:\n        Save DataFrame to CSV:\n        &gt;&gt;&gt; save_local(\n        ...     'data.csv',\n        ...     df,\n        ...     index=False,\n        ...     encoding='utf-8'\n        ... )\n\n        Save multiple DataFrames to Excel sheets:\n        &gt;&gt;&gt; sheets = {\n        ...     'Sales': sales_df,\n        ...     'Costs': costs_df\n        ... }\n        &gt;&gt;&gt; save_local('report.xlsx', sheets)\n\n        Save matplotlib figure:\n        &gt;&gt;&gt; plt.plot([1, 2, 3])\n        &gt;&gt;&gt; save_local('plot.png', dpi=300)\n\n    Notes:\n        - Parent directories will be created if they don't exist\n        - For Excel files with multiple sheets, sheet names are truncated to 31 chars\n    \"\"\"\n    # Convert string path to Path object and ensure parent directory exists\n    path = Path(filepath)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Get file extension\n    extension = path.suffix.lower()[1:]\n\n    # Handle different file types\n    if extension == \"csv\":\n        if not isinstance(obj, pd.DataFrame):\n            raise TypeError(\"Object must be a pandas DataFrame for CSV export\")\n        obj.to_csv(path, **kwargs)\n\n    elif extension in [\"xlsx\", \"xls\"]:\n        if isinstance(obj, dict):\n            with pd.ExcelWriter(path, engine=\"xlsxwriter\") as writer:\n                for key_obj, value_obj in obj.items():\n                    # Excel sheet names limited to 31 characters\n                    export_key = key_obj if len(key_obj) &lt;= 31 else key_obj[:31]\n                    value_obj.to_excel(writer, sheet_name=export_key, **kwargs)\n        elif isinstance(obj, pd.DataFrame):\n            with pd.ExcelWriter(path, engine=\"xlsxwriter\") as writer:\n                obj.to_excel(writer, **kwargs)\n        else:\n            raise TypeError(\n                \"Object must be a DataFrame or dict of DataFrames for Excel export\"\n            )\n\n    elif extension == \"json\":\n        if isinstance(obj, pd.DataFrame):\n            obj.to_json(path, **kwargs)\n        else:\n            with open(path, \"w\") as f:\n                json.dump(obj, f, **kwargs)\n\n    elif extension == \"pkl\":\n        with open(path, \"wb\") as f:\n            dump(obj, f, **kwargs)\n\n    elif extension == \"png\":\n        plt.savefig(path, format=\"png\", **kwargs)\n        plt.close(\"all\")\n\n    elif extension == \"parquet\":\n        if not isinstance(obj, pd.DataFrame):\n            raise TypeError(\"Object must be a pandas DataFrame for Parquet export\")\n        obj.to_parquet(path, **kwargs)\n\n    elif extension == \"geojson\":\n        if not isinstance(obj, gpd.GeoDataFrame):\n            raise TypeError(\"Object must be a GeoDataFrame for GeoJSON export\")\n        obj.to_file(path, driver=\"GeoJSON\", **kwargs)\n\n    else:\n        raise ValueError(\n            \"File type should either be csv, xlsx, xls, json, pkl, parquet, geojson or png.\"\n        )\n</code></pre>"},{"location":"api/storage/local/save_local/#dashboard_template_database.storage.local.saver.save_local(filepath)","title":"<code>filepath</code>","text":""},{"location":"api/storage/local/save_local/#dashboard_template_database.storage.local.saver.save_local(obj)","title":"<code>obj</code>","text":""},{"location":"api/storage/local/save_local/#dashboard_template_database.storage.local.saver.save_local(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/storage/s3/S3Loader/","title":"S3Loader","text":""},{"location":"api/storage/s3/S3Loader/#dashboard_template_database.storage.s3.loader.S3Loader","title":"S3Loader","text":"<p>A class for loading data from Amazon S3 buckets.</p> <p>This class extends the <code>_S3Connection</code> parent class and provides methods for establishing connections to S3 buckets and loading data from S3 objects.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Package to use for S3 connections ('s3fs' or 'boto3'). Defaults to \"boto3\".</p> <code>'boto3'</code> <p>Attributes:</p> Name Type Description <code>s3</code> <p>The S3 connection object (initialized when needed)</p> <code>s3_package</code> <code>str</code> <p>The package being used for S3 connectivity</p> <p>Examples:</p> <p>Load CSV from S3 using boto3:</p> <pre><code>&gt;&gt;&gt; loader = S3Loader()\n&gt;&gt;&gt; loader.connect(\n...     aws_access_key_id='YOUR_KEY',\n...     aws_secret_access_key='YOUR_SECRET'\n... )\n&gt;&gt;&gt; data = loader.load(\n...     bucket='my-bucket',\n...     key='path/to/file.csv'\n... )\n</code></pre> <p>Load Excel file using s3fs:</p> <pre><code>&gt;&gt;&gt; loader = S3Loader(s3_package='s3fs')\n&gt;&gt;&gt; loader.connect()  # Uses environment variables\n&gt;&gt;&gt; data = loader.load(\n...     bucket='my-bucket',\n...     key='path/to/file.xlsx',\n...     sheet_name='Data'\n... )\n</code></pre> <p>Methods:</p> Name Description <code>connect</code> <p>Establish a connection to the S3 bucket.</p> <code>load</code> <p>Load data from a specified S3 object based on its file extension.</p> Source code in <code>dashboard_template_database/storage/s3/loader.py</code> <pre><code>class S3Loader(_S3Connection):\n    \"\"\"A class for loading data from Amazon S3 buckets.\n\n    This class extends the `_S3Connection` parent class and provides methods for\n    establishing connections to S3 buckets and loading data from S3 objects.\n\n    Args:\n        s3_package (str, optional): Package to use for S3 connections ('s3fs' or 'boto3').\n            Defaults to \"boto3\".\n\n    Attributes:\n        s3: The S3 connection object (initialized when needed)\n        s3_package (str): The package being used for S3 connectivity\n\n    Examples:\n        Load CSV from S3 using boto3:\n        &gt;&gt;&gt; loader = S3Loader()\n        &gt;&gt;&gt; loader.connect(\n        ...     aws_access_key_id='YOUR_KEY',\n        ...     aws_secret_access_key='YOUR_SECRET'\n        ... )\n        &gt;&gt;&gt; data = loader.load(\n        ...     bucket='my-bucket',\n        ...     key='path/to/file.csv'\n        ... )\n\n        Load Excel file using s3fs:\n        &gt;&gt;&gt; loader = S3Loader(s3_package='s3fs')\n        &gt;&gt;&gt; loader.connect()  # Uses environment variables\n        &gt;&gt;&gt; data = loader.load(\n        ...     bucket='my-bucket',\n        ...     key='path/to/file.xlsx',\n        ...     sheet_name='Data'\n        ... )\n    \"\"\"\n\n    def __init__(self, s3_package: Optional[str] = \"boto3\") -&gt; None:\n        \"\"\"Initialize the S3Loader with specified S3 package.\n\n        Args:\n            s3_package (str, optional): Package to use for S3 connections.\n                Must be either 's3fs' or 'boto3'. Defaults to \"boto3\".\n        \"\"\"\n        # Initialisation du parent\n        super().__init__(s3_package=s3_package)\n\n    def connect(self, **kwargs) -&gt; None:\n        \"\"\"\n        Establish a connection to the S3 bucket.\n\n        Args:\n            **kwargs: Additional keyword arguments for establishing the connection.\n\n        Returns:\n            object: The established S3 connection.\n\n        Example :\n        &gt;&gt;&gt; s3_loader = S3Loader(package='boto3')\n        &gt;&gt;&gt; s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n        \"\"\"\n        # Etablissement d'une connection\n        return self._connect(**kwargs)\n\n    def load(self, bucket: str, key: str, **kwargs) -&gt; None:\n        \"\"\"\n        Load data from a specified S3 object based on its file extension.\n\n        Args:\n            bucket (str): The name of the S3 bucket.\n            key (str): The key of the S3 object to load.\n            **kwargs: Additional keyword arguments for reading the data.\n\n        Returns:\n            object: The loaded data (Pandas DataFrame, JSON object, Pickle object, or GeoDataFrame).\n\n        Example :\n        &gt;&gt;&gt; s3_loader = S3Loader(package='boto3')\n        &gt;&gt;&gt; s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n        &gt;&gt;&gt; data = s3_loader.load(bucket='your_bucket', key='your_file.csv')\n        \"\"\"\n        # Etablissement d'une connexion s'il n'en existe pas une nouvelle\n        if not hasattr(self, \"s3\"):\n            self.connect()\n\n        # Extraction de l'extension du fichier \u00e0 charger\n        extension = key.split(\".\")[-1]\n\n        # Chargement des donn\u00e9es\n        if self.s3_package == \"boto3\":\n            # Ouverture du fichier\n            s3_file = self.s3.get_object(Bucket=bucket, Key=key)[\"Body\"]\n            # Test suivant l'extension du fichier \u00e0 charger et lecture de ce-dernier\n            if extension == \"xlsx\":\n                data = pd.read_excel(s3_file.read(), engine=\"openpyxl\", **kwargs)\n            elif extension == \"xls\":\n                data = pd.read_excel(s3_file.read(), engine=\"xlrd\", **kwargs)\n            elif extension == \"parquet\":\n                data = pd.read_parquet(BytesIO(s3_file.read()), **kwargs)\n            else:\n                data = self._read_data(s3_file=s3_file, extension=extension, **kwargs)\n        elif self.s3_package == \"s3fs\":\n            with self.s3.open(f\"{bucket}/{key}\", \"rb\") as s3_file:\n                # Test suivant l'extension du fichier \u00e0 charger et lecture de ce-dernier\n                if extension == \"xlsx\":\n                    data = pd.read_excel(s3_file, engine=\"openpyxl\", **kwargs)\n                elif extension == \"xls\":\n                    data = pd.read_excel(s3_file, engine=\"xlrd\", **kwargs)\n                else:\n                    data = self._read_data(\n                        s3_file=s3_file, extension=extension, **kwargs\n                    )\n\n        return data\n\n    # Fonction auxiliaire de lecture des donn\u00e9es\n    def _read_data(self, s3_file, extension: str, **kwargs):\n        \"\"\"Read data from an S3 file based on its extension.\n\n        Internal method to handle reading of data from S3 files based on their format.\n\n        Args:\n            s3_file: The S3 file object to read from (type varies by s3_package)\n            extension (str): File extension indicating format\n            **kwargs: Additional arguments passed to the reading function\n\n        Returns:\n            object: The loaded data in appropriate format:\n                - .csv -&gt; pandas DataFrame\n                - .json -&gt; dict or pandas DataFrame\n                - .pkl -&gt; pickled object\n                - .geojson -&gt; GeoDataFrame\n                - .parquet -&gt; pandas DataFrame\n\n        Raises:\n            ValueError: If the extension is not supported\n            pd.errors.EmptyDataError: If the file is empty\n            json.JSONDecodeError: If JSON file is invalid\n\n        Example:\n            &gt;&gt;&gt; s3_file = s3.get_object(Bucket='bucket', Key='file.csv')['Body']\n            &gt;&gt;&gt; data = loader._read_data(s3_file, 'csv', encoding='utf-8')\n        \"\"\"\n        # Test sur l'extension et lecture du fichier\n        if extension == \"csv\":\n            data = pd.read_csv(s3_file, **kwargs)\n        elif extension == \"json\":\n            data = json.load(s3_file, **kwargs)\n        elif extension == \"pkl\":\n            data = pd.read_pickle(s3_file, **kwargs)\n        elif extension == \"geojson\":\n            data = read_file(s3_file, **kwargs)\n        elif extension == \"parquet\":\n            data = pd.read_parquet(s3_file, **kwargs)\n        else:\n            raise ValueError(\n                \"Invalid extension : should be in ['xlsx', 'xls', 'json', 'pkl', 'geojson', 'csv', 'parquet'].\"\n            )\n        return data\n</code></pre>"},{"location":"api/storage/s3/S3Loader/#dashboard_template_database.storage.s3.loader.S3Loader(s3_package)","title":"<code>s3_package</code>","text":""},{"location":"api/storage/s3/S3Loader/#dashboard_template_database.storage.s3.loader.S3Loader.connect","title":"connect","text":"<pre><code>connect(**kwargs) -&gt; None\n</code></pre> <p>Establish a connection to the S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <p>Additional keyword arguments for establishing the connection.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>object</code> <code>None</code> <p>The established S3 connection.</p> <p>Example :</p> <p>s3_loader = S3Loader(package='boto3') s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')</p> Source code in <code>dashboard_template_database/storage/s3/loader.py</code> <pre><code>def connect(self, **kwargs) -&gt; None:\n    \"\"\"\n    Establish a connection to the S3 bucket.\n\n    Args:\n        **kwargs: Additional keyword arguments for establishing the connection.\n\n    Returns:\n        object: The established S3 connection.\n\n    Example :\n    &gt;&gt;&gt; s3_loader = S3Loader(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n    \"\"\"\n    # Etablissement d'une connection\n    return self._connect(**kwargs)\n</code></pre>"},{"location":"api/storage/s3/S3Loader/#dashboard_template_database.storage.s3.loader.S3Loader.connect(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/storage/s3/S3Loader/#dashboard_template_database.storage.s3.loader.S3Loader.load","title":"load","text":"<pre><code>load(bucket: str, key: str, **kwargs) -&gt; None\n</code></pre> <p>Load data from a specified S3 object based on its file extension.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the S3 bucket.</p> required <code>str</code> <p>The key of the S3 object to load.</p> required <p>Additional keyword arguments for reading the data.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>object</code> <code>None</code> <p>The loaded data (Pandas DataFrame, JSON object, Pickle object, or GeoDataFrame).</p> <p>Example :</p> <p>s3_loader = S3Loader(package='boto3') s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key') data = s3_loader.load(bucket='your_bucket', key='your_file.csv')</p> Source code in <code>dashboard_template_database/storage/s3/loader.py</code> <pre><code>def load(self, bucket: str, key: str, **kwargs) -&gt; None:\n    \"\"\"\n    Load data from a specified S3 object based on its file extension.\n\n    Args:\n        bucket (str): The name of the S3 bucket.\n        key (str): The key of the S3 object to load.\n        **kwargs: Additional keyword arguments for reading the data.\n\n    Returns:\n        object: The loaded data (Pandas DataFrame, JSON object, Pickle object, or GeoDataFrame).\n\n    Example :\n    &gt;&gt;&gt; s3_loader = S3Loader(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n    &gt;&gt;&gt; data = s3_loader.load(bucket='your_bucket', key='your_file.csv')\n    \"\"\"\n    # Etablissement d'une connexion s'il n'en existe pas une nouvelle\n    if not hasattr(self, \"s3\"):\n        self.connect()\n\n    # Extraction de l'extension du fichier \u00e0 charger\n    extension = key.split(\".\")[-1]\n\n    # Chargement des donn\u00e9es\n    if self.s3_package == \"boto3\":\n        # Ouverture du fichier\n        s3_file = self.s3.get_object(Bucket=bucket, Key=key)[\"Body\"]\n        # Test suivant l'extension du fichier \u00e0 charger et lecture de ce-dernier\n        if extension == \"xlsx\":\n            data = pd.read_excel(s3_file.read(), engine=\"openpyxl\", **kwargs)\n        elif extension == \"xls\":\n            data = pd.read_excel(s3_file.read(), engine=\"xlrd\", **kwargs)\n        elif extension == \"parquet\":\n            data = pd.read_parquet(BytesIO(s3_file.read()), **kwargs)\n        else:\n            data = self._read_data(s3_file=s3_file, extension=extension, **kwargs)\n    elif self.s3_package == \"s3fs\":\n        with self.s3.open(f\"{bucket}/{key}\", \"rb\") as s3_file:\n            # Test suivant l'extension du fichier \u00e0 charger et lecture de ce-dernier\n            if extension == \"xlsx\":\n                data = pd.read_excel(s3_file, engine=\"openpyxl\", **kwargs)\n            elif extension == \"xls\":\n                data = pd.read_excel(s3_file, engine=\"xlrd\", **kwargs)\n            else:\n                data = self._read_data(\n                    s3_file=s3_file, extension=extension, **kwargs\n                )\n\n    return data\n</code></pre>"},{"location":"api/storage/s3/S3Loader/#dashboard_template_database.storage.s3.loader.S3Loader.load(bucket)","title":"<code>bucket</code>","text":""},{"location":"api/storage/s3/S3Loader/#dashboard_template_database.storage.s3.loader.S3Loader.load(key)","title":"<code>key</code>","text":""},{"location":"api/storage/s3/S3Loader/#dashboard_template_database.storage.s3.loader.S3Loader.load(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/storage/s3/S3Saver/","title":"S3Saver","text":""},{"location":"api/storage/s3/S3Saver/#dashboard_template_database.storage.s3.saver.S3Saver","title":"S3Saver","text":"<p>A class for saving data to Amazon S3 buckets.</p> <p>This class extends the <code>_S3Connection</code> parent class and provides methods for establishing connections to S3 buckets and saving data to S3 objects.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Package to use for S3 connections ('s3fs' or 'boto3'). Defaults to \"boto3\".</p> <code>'boto3'</code> <p>Attributes:</p> Name Type Description <code>s3</code> <p>The S3 connection object (initialized when needed)</p> <code>s3_package</code> <code>str</code> <p>The package being used for S3 connectivity</p> <p>Examples:</p> <p>Save DataFrame to S3 using boto3:</p> <pre><code>&gt;&gt;&gt; saver = S3Saver()\n&gt;&gt;&gt; saver.connect(\n...     aws_access_key_id='YOUR_KEY',\n...     aws_secret_access_key='YOUR_SECRET'\n... )\n&gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n&gt;&gt;&gt; saver.save(\n...     bucket='my-bucket',\n...     key='path/to/file.csv',\n...     obj=df\n... )\n</code></pre> <p>Save multiple DataFrames to Excel using s3fs:</p> <pre><code>&gt;&gt;&gt; saver = S3Saver(s3_package='s3fs')\n&gt;&gt;&gt; saver.connect()  # Uses environment variables\n&gt;&gt;&gt; sheets = {'Sheet1': df1, 'Sheet2': df2}\n&gt;&gt;&gt; saver.save(\n...     bucket='my-bucket',\n...     key='path/to/file.xlsx',\n...     obj=sheets\n... )\n</code></pre> <p>Methods:</p> Name Description <code>connect</code> <p>Establish a connection to the S3 bucket.</p> <code>save</code> <p>Save an object to a specified S3 object based on its file extension and object type.</p> Source code in <code>dashboard_template_database/storage/s3/saver.py</code> <pre><code>class S3Saver(_S3Connection):\n    \"\"\"A class for saving data to Amazon S3 buckets.\n\n    This class extends the `_S3Connection` parent class and provides methods for\n    establishing connections to S3 buckets and saving data to S3 objects.\n\n    Args:\n        s3_package (str, optional): Package to use for S3 connections ('s3fs' or 'boto3').\n            Defaults to \"boto3\".\n\n    Attributes:\n        s3: The S3 connection object (initialized when needed)\n        s3_package (str): The package being used for S3 connectivity\n\n    Examples:\n        Save DataFrame to S3 using boto3:\n        &gt;&gt;&gt; saver = S3Saver()\n        &gt;&gt;&gt; saver.connect(\n        ...     aws_access_key_id='YOUR_KEY',\n        ...     aws_secret_access_key='YOUR_SECRET'\n        ... )\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n        &gt;&gt;&gt; saver.save(\n        ...     bucket='my-bucket',\n        ...     key='path/to/file.csv',\n        ...     obj=df\n        ... )\n\n        Save multiple DataFrames to Excel using s3fs:\n        &gt;&gt;&gt; saver = S3Saver(s3_package='s3fs')\n        &gt;&gt;&gt; saver.connect()  # Uses environment variables\n        &gt;&gt;&gt; sheets = {'Sheet1': df1, 'Sheet2': df2}\n        &gt;&gt;&gt; saver.save(\n        ...     bucket='my-bucket',\n        ...     key='path/to/file.xlsx',\n        ...     obj=sheets\n        ... )\n    \"\"\"\n\n    def __init__(self, s3_package: Optional[str] = \"boto3\") -&gt; None:\n        \"\"\"Initialize the S3Saver with specified S3 package.\n\n        Args:\n            s3_package (str, optional): Package to use for S3 connections.\n                Must be either 's3fs' or 'boto3'. Defaults to \"boto3\".\n        \"\"\"\n        # Initialisation du parent\n        super().__init__(s3_package=s3_package)\n\n    def connect(self, **kwargs) -&gt; None:\n        \"\"\"\n        Establish a connection to the S3 bucket.\n\n        Args:\n            **kwargs: Additional keyword arguments for establishing the connection.\n\n        Returns:\n            object: The established S3 connection.\n\n        Example usage:\n        &gt;&gt;&gt; s3_saver = S3Saver(package='boto3')\n        &gt;&gt;&gt; s3_connection = s3_saver.connect(\n            aws_access_key_id='your_access_key',\n            aws_secret_access_key='your_secret_key'\n        )\n        \"\"\"\n        # Etablissement d'une connection\n        return self._connect(**kwargs)\n\n    def save(\n        self, bucket: str, key: str, obj: Optional[Union[object, None]] = None, **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Save an object to a specified S3 object based on its file extension and object type.\n\n        Args:\n            bucket (str): The name of the S3 bucket.\n            key (str): The key of the S3 object to save.\n            obj (object): The object to save (DataFrame, dictionary, etc.).\n            **kwargs: Additional keyword arguments for saving the object.\n\n        Raises:\n            ValueError: If the file extension is not supported\n            TypeError: If the object type doesn't match the requirements\n\n        Example :\n        &gt;&gt;&gt; s3_saver = S3Saver(package='boto3')\n        &gt;&gt;&gt; s3_connection = s3_saver.connect(\n            aws_access_key_id='your_access_key',\n            aws_secret_access_key='your_secret_key'\n        )\n        &gt;&gt;&gt; s3_saver.save(bucket='your_bucket', key='your_file.csv', obj=dataframe)\n        \"\"\"\n        # Etablissement d'une connexion s'il n'en existe pas une nouvelle\n        if not hasattr(self, \"s3\"):\n            self.connect()\n\n        # Extraction de l'extension du fichier \u00e0 charger\n        extension = key.split(\".\")[-1]\n\n        # Exportation de l'objet\n        if self.s3_package == \"boto3\":\n            if extension == \"csv\":\n                self.s3.put_object(Bucket=bucket, Key=key, Body=obj.to_csv(**kwargs))\n            elif extension in [\"xlsx\", \"xls\"]:\n                # Si l'objet est un dictionnaire de DataFrame, un jeu de donn\u00e9es est export\u00e9 par feuille\n                if isinstance(obj, dict):\n                    # Construction de l'objet \u00e0 exporter\n                    with BytesIO() as output:\n                        with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                            for key_obj, value_obj in obj.items():\n                                # La longueur d'une sheet_name est major\u00e9 \u00e0 31 caract\u00e8res\n                                export_key = (\n                                    key_obj if len(key_obj) &lt;= 31 else key_obj[:31]\n                                )\n                                value_obj.to_excel(\n                                    writer, sheet_name=export_key, **kwargs\n                                )\n                        output_data = output.getvalue()\n                elif isinstance(obj, pd.DataFrame):\n                    with BytesIO() as output:\n                        with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                            obj.to_excel(writer, **kwargs)\n                        output_data = output.getvalue()\n                # Exportation de l'objet\n                self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n            elif extension == \"json\":\n                if isinstance(obj, pd.DataFrame):\n                    self.s3.put_object(\n                        Bucket=bucket, Key=key, Body=obj.to_json(**kwargs)\n                    )\n                else:\n                    self.s3.put_object(Bucket=bucket, Key=key, Body=dumps(obj))\n            elif extension == \"pkl\":\n                with BytesIO() as output:\n                    dump(obj, output)\n                    output_data = output.getvalue()\n                self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n            elif extension == \"png\":\n                # Construction de l'objet \u00e0 exporter\n                with BytesIO() as output:\n                    savefig(output, format=\"png\", **kwargs)\n                    output_data = output.getvalue()\n                # Exportation de l'objet\n                self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n                # Fermeture des figures\n                close(\"all\")\n            elif extension == \"parquet\":\n                # Construction de l'objet \u00e0 exporter\n                with BytesIO() as output:\n                    obj.to_parquet(output, **kwargs)\n            elif extension == \"geojson\":\n                self.s3.put_object(\n                    Bucket=bucket, Key=key, Body=obj.to_json().encode(\"utf-8\")\n                )\n            else:\n                raise ValueError(\n                    \"File type should either be csv, xlsx, xls, json, pkl, geojson or png.\"\n                )\n\n        elif self.s3_package == \"s3fs\":\n            # Distinction suivant le format du fichier et export\n            if extension in [\"xlsx\", \"xls\"]:\n                with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                    # Si l'objet est un dictionnaire de DataFrame, un jeu de donn\u00e9es est export\u00e9 par feuille\n                    if isinstance(obj, dict):\n                        # Construction de l'objet \u00e0 exporter\n                        with BytesIO() as output:\n                            with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                                for key_obj, value_obj in obj.items():\n                                    # La longueur d'une sheet_name est major\u00e9e \u00e0 31 caract\u00e8res\n                                    export_key = (\n                                        key_obj if len(key_obj) &lt;= 31 else key_obj[:31]\n                                    )\n                                    value_obj.to_excel(\n                                        writer, sheet_name=export_key, **kwargs\n                                    )\n                            output_data = output.getvalue()\n                        # Exportation de l'objet\n                        s3_file.write(output_data)\n                    elif isinstance(obj, pd.DataFrame):\n                        # Exportation de l'objet\n                        with pd.ExcelWriter(s3_file, engine=\"xlsxwriter\") as writer:\n                            obj.to_excel(writer, **kwargs)\n            elif extension == \"parquet\":\n                with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                    obj.to_parquet(s3_file)\n            elif extension == \"png\":\n                with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                    # Construction de l'objet \u00e0 exporter\n                    with BytesIO() as output:\n                        savefig(output, format=\"png\", **kwargs)\n                        output_data = output.getvalue()\n                    # Exportation de l'objet\n                    s3_file.write(output_data)\n                    # Fermeture des figures\n                    close(\"all\")\n            else:\n                with self.s3.open(f\"{bucket}/{key}\", \"w\") as s3_file:\n                    # Distinction suivant le format du fichier et export\n                    if extension == \"csv\":\n                        obj.to_csv(s3_file, **kwargs)\n                    elif extension == \"json\":\n                        if isinstance(obj, pd.DataFrame):\n                            s3_file.write(obj.to_json(**kwargs))\n                        else:\n                            s3_file.write(dumps(obj))\n                    elif extension == \"pkl\":\n                        obj.to_pickle(s3_file, **kwargs)\n                    elif extension == \"geojson\":\n                        obj.to_file(s3_file, **kwargs)\n                    else:\n                        raise ValueError(\n                            \"File type should either be csv, xlsx, xls, json, pkl, parquet, geojson or png.\"\n                        )\n</code></pre>"},{"location":"api/storage/s3/S3Saver/#dashboard_template_database.storage.s3.saver.S3Saver(s3_package)","title":"<code>s3_package</code>","text":""},{"location":"api/storage/s3/S3Saver/#dashboard_template_database.storage.s3.saver.S3Saver.connect","title":"connect","text":"<pre><code>connect(**kwargs) -&gt; None\n</code></pre> <p>Establish a connection to the S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <p>Additional keyword arguments for establishing the connection.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>object</code> <code>None</code> <p>The established S3 connection.</p> <p>Example usage:</p> <p>s3_saver = S3Saver(package='boto3') s3_connection = s3_saver.connect(     aws_access_key_id='your_access_key',     aws_secret_access_key='your_secret_key' )</p> Source code in <code>dashboard_template_database/storage/s3/saver.py</code> <pre><code>def connect(self, **kwargs) -&gt; None:\n    \"\"\"\n    Establish a connection to the S3 bucket.\n\n    Args:\n        **kwargs: Additional keyword arguments for establishing the connection.\n\n    Returns:\n        object: The established S3 connection.\n\n    Example usage:\n    &gt;&gt;&gt; s3_saver = S3Saver(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_saver.connect(\n        aws_access_key_id='your_access_key',\n        aws_secret_access_key='your_secret_key'\n    )\n    \"\"\"\n    # Etablissement d'une connection\n    return self._connect(**kwargs)\n</code></pre>"},{"location":"api/storage/s3/S3Saver/#dashboard_template_database.storage.s3.saver.S3Saver.connect(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/storage/s3/S3Saver/#dashboard_template_database.storage.s3.saver.S3Saver.save","title":"save","text":"<pre><code>save(bucket: str, key: str, obj: Optional[Union[object, None]] = None, **kwargs) -&gt; None\n</code></pre> <p>Save an object to a specified S3 object based on its file extension and object type.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the S3 bucket.</p> required <code>str</code> <p>The key of the S3 object to save.</p> required <code>object</code> <p>The object to save (DataFrame, dictionary, etc.).</p> <code>None</code> <p>Additional keyword arguments for saving the object.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file extension is not supported</p> <code>TypeError</code> <p>If the object type doesn't match the requirements</p> <p>Example :</p> <p>s3_saver = S3Saver(package='boto3') s3_connection = s3_saver.connect(     aws_access_key_id='your_access_key',     aws_secret_access_key='your_secret_key' ) s3_saver.save(bucket='your_bucket', key='your_file.csv', obj=dataframe)</p> Source code in <code>dashboard_template_database/storage/s3/saver.py</code> <pre><code>def save(\n    self, bucket: str, key: str, obj: Optional[Union[object, None]] = None, **kwargs\n) -&gt; None:\n    \"\"\"\n    Save an object to a specified S3 object based on its file extension and object type.\n\n    Args:\n        bucket (str): The name of the S3 bucket.\n        key (str): The key of the S3 object to save.\n        obj (object): The object to save (DataFrame, dictionary, etc.).\n        **kwargs: Additional keyword arguments for saving the object.\n\n    Raises:\n        ValueError: If the file extension is not supported\n        TypeError: If the object type doesn't match the requirements\n\n    Example :\n    &gt;&gt;&gt; s3_saver = S3Saver(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_saver.connect(\n        aws_access_key_id='your_access_key',\n        aws_secret_access_key='your_secret_key'\n    )\n    &gt;&gt;&gt; s3_saver.save(bucket='your_bucket', key='your_file.csv', obj=dataframe)\n    \"\"\"\n    # Etablissement d'une connexion s'il n'en existe pas une nouvelle\n    if not hasattr(self, \"s3\"):\n        self.connect()\n\n    # Extraction de l'extension du fichier \u00e0 charger\n    extension = key.split(\".\")[-1]\n\n    # Exportation de l'objet\n    if self.s3_package == \"boto3\":\n        if extension == \"csv\":\n            self.s3.put_object(Bucket=bucket, Key=key, Body=obj.to_csv(**kwargs))\n        elif extension in [\"xlsx\", \"xls\"]:\n            # Si l'objet est un dictionnaire de DataFrame, un jeu de donn\u00e9es est export\u00e9 par feuille\n            if isinstance(obj, dict):\n                # Construction de l'objet \u00e0 exporter\n                with BytesIO() as output:\n                    with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                        for key_obj, value_obj in obj.items():\n                            # La longueur d'une sheet_name est major\u00e9 \u00e0 31 caract\u00e8res\n                            export_key = (\n                                key_obj if len(key_obj) &lt;= 31 else key_obj[:31]\n                            )\n                            value_obj.to_excel(\n                                writer, sheet_name=export_key, **kwargs\n                            )\n                    output_data = output.getvalue()\n            elif isinstance(obj, pd.DataFrame):\n                with BytesIO() as output:\n                    with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                        obj.to_excel(writer, **kwargs)\n                    output_data = output.getvalue()\n            # Exportation de l'objet\n            self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n        elif extension == \"json\":\n            if isinstance(obj, pd.DataFrame):\n                self.s3.put_object(\n                    Bucket=bucket, Key=key, Body=obj.to_json(**kwargs)\n                )\n            else:\n                self.s3.put_object(Bucket=bucket, Key=key, Body=dumps(obj))\n        elif extension == \"pkl\":\n            with BytesIO() as output:\n                dump(obj, output)\n                output_data = output.getvalue()\n            self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n        elif extension == \"png\":\n            # Construction de l'objet \u00e0 exporter\n            with BytesIO() as output:\n                savefig(output, format=\"png\", **kwargs)\n                output_data = output.getvalue()\n            # Exportation de l'objet\n            self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n            # Fermeture des figures\n            close(\"all\")\n        elif extension == \"parquet\":\n            # Construction de l'objet \u00e0 exporter\n            with BytesIO() as output:\n                obj.to_parquet(output, **kwargs)\n        elif extension == \"geojson\":\n            self.s3.put_object(\n                Bucket=bucket, Key=key, Body=obj.to_json().encode(\"utf-8\")\n            )\n        else:\n            raise ValueError(\n                \"File type should either be csv, xlsx, xls, json, pkl, geojson or png.\"\n            )\n\n    elif self.s3_package == \"s3fs\":\n        # Distinction suivant le format du fichier et export\n        if extension in [\"xlsx\", \"xls\"]:\n            with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                # Si l'objet est un dictionnaire de DataFrame, un jeu de donn\u00e9es est export\u00e9 par feuille\n                if isinstance(obj, dict):\n                    # Construction de l'objet \u00e0 exporter\n                    with BytesIO() as output:\n                        with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                            for key_obj, value_obj in obj.items():\n                                # La longueur d'une sheet_name est major\u00e9e \u00e0 31 caract\u00e8res\n                                export_key = (\n                                    key_obj if len(key_obj) &lt;= 31 else key_obj[:31]\n                                )\n                                value_obj.to_excel(\n                                    writer, sheet_name=export_key, **kwargs\n                                )\n                        output_data = output.getvalue()\n                    # Exportation de l'objet\n                    s3_file.write(output_data)\n                elif isinstance(obj, pd.DataFrame):\n                    # Exportation de l'objet\n                    with pd.ExcelWriter(s3_file, engine=\"xlsxwriter\") as writer:\n                        obj.to_excel(writer, **kwargs)\n        elif extension == \"parquet\":\n            with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                obj.to_parquet(s3_file)\n        elif extension == \"png\":\n            with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                # Construction de l'objet \u00e0 exporter\n                with BytesIO() as output:\n                    savefig(output, format=\"png\", **kwargs)\n                    output_data = output.getvalue()\n                # Exportation de l'objet\n                s3_file.write(output_data)\n                # Fermeture des figures\n                close(\"all\")\n        else:\n            with self.s3.open(f\"{bucket}/{key}\", \"w\") as s3_file:\n                # Distinction suivant le format du fichier et export\n                if extension == \"csv\":\n                    obj.to_csv(s3_file, **kwargs)\n                elif extension == \"json\":\n                    if isinstance(obj, pd.DataFrame):\n                        s3_file.write(obj.to_json(**kwargs))\n                    else:\n                        s3_file.write(dumps(obj))\n                elif extension == \"pkl\":\n                    obj.to_pickle(s3_file, **kwargs)\n                elif extension == \"geojson\":\n                    obj.to_file(s3_file, **kwargs)\n                else:\n                    raise ValueError(\n                        \"File type should either be csv, xlsx, xls, json, pkl, parquet, geojson or png.\"\n                    )\n</code></pre>"},{"location":"api/storage/s3/S3Saver/#dashboard_template_database.storage.s3.saver.S3Saver.save(bucket)","title":"<code>bucket</code>","text":""},{"location":"api/storage/s3/S3Saver/#dashboard_template_database.storage.s3.saver.S3Saver.save(key)","title":"<code>key</code>","text":""},{"location":"api/storage/s3/S3Saver/#dashboard_template_database.storage.s3.saver.S3Saver.save(obj)","title":"<code>obj</code>","text":""},{"location":"api/storage/s3/S3Saver/#dashboard_template_database.storage.s3.saver.S3Saver.save(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/storage/s3/_S3Connection/","title":"_S3Connection","text":""},{"location":"api/storage/s3/_S3Connection/#dashboard_template_database.storage.s3._connection._S3Connection","title":"_S3Connection","text":"<p>Base class for managing connections to Amazon S3 buckets.</p> <p>This class provides the foundational functionality for connecting to S3 buckets using either 's3fs' or 'boto3' as the underlying package.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Package to use for S3 connections ('s3fs' or 'boto3'). Defaults to \"boto3\".</p> <code>'boto3'</code> <p>Attributes:</p> Name Type Description <code>s3_package</code> <code>str</code> <p>The package being used for S3 connectivity</p> <code>s3</code> <code>str</code> <p>The S3 connection object (initialized when needed)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If s3_package is not 's3fs' or 'boto3'</p> <p>Examples:</p> <p>Using boto3:</p> <pre><code>&gt;&gt;&gt; conn = _S3Connection()\n&gt;&gt;&gt; conn._connect(\n...     aws_access_key_id='YOUR_KEY',\n...     aws_secret_access_key='YOUR_SECRET'\n... )\n</code></pre> <p>Using s3fs with custom endpoint:</p> <pre><code>&gt;&gt;&gt; conn = _S3Connection(s3_package='s3fs')\n&gt;&gt;&gt; conn._connect(endpoint_url='https://custom.endpoint')\n</code></pre> Notes <ul> <li>AWS credentials can be provided either through environment variables or   as parameters during connection.</li> <li>Environment variables used:<ul> <li>AWS_S3_ENDPOINT</li> <li>AWS_ACCESS_KEY_ID</li> <li>AWS_SECRET_ACCESS_KEY</li> <li>AWS_SESSION_TOKEN</li> </ul> </li> </ul> Source code in <code>dashboard_template_database/storage/s3/_connection.py</code> <pre><code>class _S3Connection:\n    \"\"\"Base class for managing connections to Amazon S3 buckets.\n\n    This class provides the foundational functionality for connecting to S3 buckets\n    using either 's3fs' or 'boto3' as the underlying package.\n\n    Args:\n        s3_package (str, optional): Package to use for S3 connections ('s3fs' or 'boto3').\n            Defaults to \"boto3\".\n\n    Attributes:\n        s3_package (str): The package being used for S3 connectivity\n        s3: The S3 connection object (initialized when needed)\n\n    Raises:\n        ValueError: If s3_package is not 's3fs' or 'boto3'\n\n    Examples:\n        Using boto3:\n        &gt;&gt;&gt; conn = _S3Connection()\n        &gt;&gt;&gt; conn._connect(\n        ...     aws_access_key_id='YOUR_KEY',\n        ...     aws_secret_access_key='YOUR_SECRET'\n        ... )\n\n        Using s3fs with custom endpoint:\n        &gt;&gt;&gt; conn = _S3Connection(s3_package='s3fs')\n        &gt;&gt;&gt; conn._connect(endpoint_url='https://custom.endpoint')\n\n    Notes:\n        - AWS credentials can be provided either through environment variables or\n          as parameters during connection.\n        - Environment variables used:\n            - AWS_S3_ENDPOINT\n            - AWS_ACCESS_KEY_ID\n            - AWS_SECRET_ACCESS_KEY\n            - AWS_SESSION_TOKEN\n    \"\"\"\n\n    def __init__(self, s3_package: Optional[str] = \"boto3\") -&gt; None:\n        \"\"\"Initialize the S3 connection with specified S3 package.\n\n        Args:\n            s3_package (str, optional): Package to use for S3 connections.\n                Must be either 's3fs' or 'boto3'. Defaults to \"boto3\".\n        \"\"\"\n        # Initialisation du package utilis\u00e9 pour se connecter au bucket S3\n        # Deux valeurs sont valides pour ce param\u00e8tre 's3fs' et 'boto3'\n        if s3_package not in [\"s3fs\", \"boto3\"]:\n            raise ValueError(\"'s3_package' must be in ['s3fs', 'boto3']\")\n\n        self.s3_package = s3_package\n\n    def _connect(\n        self,\n        endpoint_url: Optional[Union[str, None]] = None,\n        aws_access_key_id: Optional[Union[str, None]] = None,\n        aws_secret_access_key: Optional[Union[str, None]] = None,\n        aws_session_token: Optional[Union[str, None]] = None,\n        verify: Optional[bool] = False,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Establish connection to S3 using specified credentials.\n\n        Args:\n            endpoint_url (str, optional): Custom S3 endpoint URL\n            aws_access_key_id (str, optional): AWS access key\n            aws_secret_access_key (str, optional): AWS secret key\n            aws_session_token (str, optional): AWS session token\n            verify (bool, optional): Whether to verify SSL certificates\n            **kwargs: Additional arguments passed to boto3.client or S3FileSystem\n\n        Returns:\n            _S3Connection: Self, with initialized s3 attribute\n\n        Raises:\n            ValueError: If s3_package is invalid\n            botocore.exceptions.ClientError: If connection fails\n\n        Notes:\n            If credentials are not provided, they will be read from environment\n            variables.\n        \"\"\"\n        # D\u00e9sactive les warnings en raison de la non v\u00e9rification du certificat (non recommand\u00e9)\n        disable_warnings()\n\n        if self.s3_package == \"boto3\":\n            self.s3 = client(\n                \"s3\",\n                endpoint_url=(\n                    endpoint_url\n                    if endpoint_url is not None\n                    else \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n                ),\n                aws_access_key_id=(\n                    aws_access_key_id\n                    if aws_access_key_id is not None\n                    else os.environ[\"AWS_ACCESS_KEY_ID\"]\n                ),\n                aws_secret_access_key=(\n                    aws_secret_access_key\n                    if aws_secret_access_key is not None\n                    else os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n                ),\n                aws_session_token=(\n                    aws_session_token\n                    if aws_session_token is not None\n                    else os.environ[\"AWS_SESSION_TOKEN\"]\n                ),\n                verify=verify,\n                **kwargs,\n            )\n        elif self.s3_package == \"s3fs\":\n            self.s3 = S3FileSystem(\n                client_kwargs={\n                    \"endpoint_url\": (\n                        endpoint_url\n                        if endpoint_url is not None\n                        else \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n                    )\n                },\n                **kwargs,\n            )\n        else:\n            raise ValueError(\"'s3_package' must be in ['s3fs', 'boto3']\")\n\n        return self\n</code></pre>"},{"location":"api/storage/s3/_S3Connection/#dashboard_template_database.storage.s3._connection._S3Connection(s3_package)","title":"<code>s3_package</code>","text":""},{"location":"api/utils/_init_logger/","title":"_init_logger","text":""},{"location":"api/utils/_init_logger/#dashboard_template_database.utils.logger._init_logger","title":"_init_logger","text":"<pre><code>_init_logger(filename: PathLike) -&gt; Logger\n</code></pre> <p>Initializes the logger for logging to a file.</p> <p>Parameters:</p> Name Type Description Default <code>PathLike</code> <p>Path to the log file.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>logging.Logger: Initialized logger object.</p> Note <p>This function configures logging to output messages to both console and a file.</p> Source code in <code>dashboard_template_database/utils/logger.py</code> <pre><code>def _init_logger(filename: os.PathLike) -&gt; logging.Logger:\n    \"\"\"\n    Initializes the logger for logging to a file.\n\n    Parameters:\n        filename (os.PathLike): Path to the log file.\n\n    Returns:\n        logging.Logger: Initialized logger object.\n\n    Note:\n        This function configures logging to output messages to both console and a file.\n    \"\"\"\n    # Configuration de logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n        encoding=\"utf-8\",\n        level=logging.INFO,\n    )\n\n    # V\u00e9rification de l'existance du dossier pour le fichier de log\n    log_directory = os.path.dirname(filename)\n\n    if (not os.path.exists(log_directory)) &amp; (log_directory != \"\"):\n        os.makedirs(log_directory)\n\n    # Configuration du fichier de logs\n    file_handler = logging.FileHandler(filename)\n    file_handler.setLevel(logging.INFO)\n\n    # Set a formatter for the file handler\n    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n    file_handler.setFormatter(formatter)\n\n    # Initialisation du logger\n    logger = logging.getLogger()\n    logger.addHandler(file_handler)\n\n    return logger\n</code></pre>"},{"location":"api/utils/_init_logger/#dashboard_template_database.utils.logger._init_logger(filename)","title":"<code>filename</code>","text":""}]}